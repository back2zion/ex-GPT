@echo off
echo ========================================
echo μ¤ν”μ†μ¤ LLM μ„λ²„ μ—°κ²° ν™•μΈ
echo ========================================
echo.

echo [ν™•μΈ] .env νμΌ μ΅΄μ¬ μ—¬λ¶€...
if not exist ".env" (
    echo β .env νμΌμ΄ μ—†μµλ‹λ‹¤.
    echo π“‹ .env.exampleμ„ λ³µμ‚¬ν•μ—¬ .env νμΌμ„ λ§λ“¤μ–΄μ£Όμ„Έμ”.
    echo.
    echo copy .env.example .env
    echo.
    goto :end
)

echo β… .env νμΌ λ°κ²¬
echo.

echo [ν™•μΈ] μ¤ν”μ†μ¤ LLM μ„λ²„ μ„¤μ •...

findstr "OLLAMA_BASE_URL=" .env >nul 2>&1
if %errorlevel% equ 0 (
    echo β… Ollama μ„λ²„ URL μ„¤μ •λ¨
) else (
    echo β Ollama μ„λ²„ URL μ„¤μ • μ—†μ
)

findstr "VLLM_BASE_URL=" .env >nul 2>&1
if %errorlevel% equ 0 (
    echo β… vLLM μ„λ²„ URL μ„¤μ •λ¨
) else (
    echo β vLLM μ„λ²„ URL μ„¤μ • μ—†μ  
)

echo.
echo ========================================
echo μ¤ν”μ†μ¤ LLM μ„λ²„ μ„¤μΉ λ°©λ²•:
echo ========================================
echo.
echo π“¦ Ollama (μ¶”μ² - κ°„νΈν• λ΅μ»¬ LLM):
echo    1. https://ollama.ai/download μ—μ„ λ‹¤μ΄λ΅λ“
echo    2. μ„¤μΉ ν›„: ollama pull llama2
echo    3. λ¨λΈ μ‹¤ν–‰: ollama run llama2
echo.
echo π“¦ vLLM (κ³ μ„±λ¥ μ„λΉ™):
echo    1. pip install vllm
echo    2. λ¨λΈ μ„λΉ™: python -m vllm.entrypoints.api_server --model microsoft/DialoGPT-medium
echo.
echo π“– μμ„Έν• μ„¤μ • λ°©λ²•: RAGFLOW_LLM_SETUP.md
echo ========================================
echo.

:end
pause
