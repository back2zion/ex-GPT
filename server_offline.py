#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ex-GPT ì˜¨í”„ë ˆë¯¸ìŠ¤ ì„œë²„ (ì™„ì „ ì˜¤í”„ë¼ì¸)
ì¸í„°ë„· ì—°ê²° ì—†ì´ ì‘ë™í•˜ëŠ” ê²©ë¦¬ëœ AI ì‹œìŠ¤í…œ
"""

import os
import json
import hashlib
from datetime import datetime
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import requests
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# ì˜¨í”„ë ˆë¯¸ìŠ¤ ì„¤ì •
OLLAMA_BASE_URL = 'http://localhost:11434'
OFFLINE_MODE = True

class OfflineAI:
    """ì™„ì „ ì˜¤í”„ë¼ì¸ AI ì²˜ë¦¬"""
    
    def __init__(self):
        self.models_available = self.check_local_models()
        
    def check_local_models(self):
        """ë¡œì»¬ ëª¨ë¸ í™•ì¸"""
        try:
            response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=3)
            if response.status_code == 200:
                models = response.json().get('models', [])
                return [m['name'] for m in models]
        except:
            logger.warning("Ollama ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        return []
    
    def route_query(self, query):
        """ì˜¤í”„ë¼ì¸ ì¿¼ë¦¬ ë¼ìš°íŒ…"""
        query_lower = query.lower()
        
        # ë¬¸ì„œ ê²€ìƒ‰ ê´€ë ¨
        if any(keyword in query_lower for keyword in ['ê²€ìƒ‰', 'ì°¾ì•„', 'ë¬¸ì„œ', 'ìë£Œ', 'íŒŒì¼']):
            return {
                'route': 'local_search',
                'description': 'ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰',
                'engine': 'qdrant_local'
            }
        
        # ìƒì„¸ ì„¤ëª… ìš”ì²­
        elif any(keyword in query_lower for keyword in ['ìì„¸íˆ', 'ìƒì„¸íˆ', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ì„¤ëª…']):
            return {
                'route': 'detailed_response',
                'description': 'ìƒì„¸ ì‘ë‹µ ìƒì„±',
                'engine': 'local_llm_enhanced'
            }
        
        # ì‘ì—… ìˆ˜í–‰ ìš”ì²­
        elif any(keyword in query_lower for keyword in ['ì‹¤í–‰', 'ì‘ì—…', 'ì²˜ë¦¬', 'ìƒì„±', 'ë§Œë“¤ì–´']):
            return {
                'route': 'task_execution',
                'description': 'ë¡œì»¬ ì‘ì—… ìˆ˜í–‰',
                'engine': 'local_automation'
            }
        
        # ê¸°ë³¸ ëŒ€í™”
        else:
            return {
                'route': 'direct_chat',
                'description': 'ì¼ë°˜ ëŒ€í™”',
                'engine': 'local_llm'
            }
    
    def process_query(self, query, route_info):
        """ì˜¤í”„ë¼ì¸ ì¿¼ë¦¬ ì²˜ë¦¬"""
        route = route_info['route']
        
        if route == 'local_search':
            return self.local_document_search(query)
        elif route == 'detailed_response':
            return self.detailed_local_response(query)
        elif route == 'task_execution':
            return self.local_task_execution(query)
        else:
            return self.direct_local_chat(query)
    
    def local_document_search(self, query):
        """ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰"""
        # ì‹¤ì œë¡œëŠ” Qdrantë‚˜ ë¡œì»¬ ì¸ë±ìŠ¤ ê²€ìƒ‰
        response = f"""ğŸ“‚ ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:

ì§ˆì˜: "{query}"

ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:
- ë‚´ë¶€ ê·œì •ì§‘ (2024.06)
- ì‹œìŠ¤í…œ ë§¤ë‰´ì–¼ (v2.1)
- ë³´ì•ˆ ê°€ì´ë“œë¼ì¸ (ìµœì‹ )

âš ï¸ í˜„ì¬ ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤.
ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ì—°ë™ì´ í•„ìš”í•©ë‹ˆë‹¤."""

        return self.call_local_llm(f"ë¬¸ì„œ ê²€ìƒ‰ ì§ˆë¬¸: {query}\n\n{response}")
    
    def detailed_local_response(self, query):
        """ìƒì„¸ ë¡œì»¬ ì‘ë‹µ"""
        enhanced_query = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë§¤ìš° ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {query}

ë‹¤ìŒ ê´€ì ì—ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ê¸°ë³¸ ê°œë… ë° ì •ì˜
2. êµ¬ì²´ì ì¸ ë°©ë²•ë¡ 
3. ì‹¤ì œ ì ìš© ì‚¬ë¡€
4. ì£¼ì˜ì‚¬í•­ ë° í•œê³„ì """

        return self.call_local_llm(enhanced_query)
    
    def local_task_execution(self, query):
        """ë¡œì»¬ ì‘ì—… ìˆ˜í–‰"""
        task_info = f"""ğŸ”§ ë¡œì»¬ ì‘ì—… ì²˜ë¦¬:

ìš”ì²­: "{query}"

ğŸ“‹ ì²˜ë¦¬ ê³„íš:
1. ìš”ì²­ ë¶„ì„ ì™„ë£Œ
2. ë¡œì»¬ ë¦¬ì†ŒìŠ¤ í™•ì¸
3. ë³´ì•ˆ ê²€ì¦ í†µê³¼
4. ì‘ì—… ëŒ€ê¸°ì—´ ì¶”ê°€

âš ï¸ ì‹¤ì œ ì‘ì—… ìˆ˜í–‰ì€ ì‹œìŠ¤í…œ ê´€ë¦¬ì ìŠ¹ì¸ í›„ ì§„í–‰ë©ë‹ˆë‹¤.
í˜„ì¬ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ê²©ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤."""

        return self.call_local_llm(f"ì‘ì—… ìš”ì²­: {query}\n\n{task_info}")
    
    def direct_local_chat(self, query):
        """ì§ì ‘ ë¡œì»¬ ì±„íŒ…"""
        return self.call_local_llm(query)
    
    def call_local_llm(self, prompt):
        """ë¡œì»¬ LLM í˜¸ì¶œ"""
        if not self.models_available:
            return f"ì˜¤í”„ë¼ì¸ ì‘ë‹µ: {prompt}\n\nâš ï¸ ë¡œì»¬ LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Ollama ì„œë¹„ìŠ¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        try:
            response = requests.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": "qwen3:8b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result.get('response', 'ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')
                
                # Qwen ëª¨ë¸ì˜ <think> íƒœê·¸ ì œê±°
                if '<think>' in ai_response:
                    ai_response = ai_response.split('</think>')[-1].strip()
                
                return ai_response
            else:
                return f"ë¡œì»¬ LLM ì˜¤ë¥˜: HTTP {response.status_code}"
                
        except requests.Timeout:
            return "ë¡œì»¬ LLM ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (30ì´ˆ). ëª¨ë¸ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"ë¡œì»¬ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}"

# ì˜¤í”„ë¼ì¸ AI ì¸ìŠ¤í„´ìŠ¤
offline_ai = OfflineAI()

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return send_from_directory('.', 'index.html')

@app.route('/<path:filename>')
def static_files(filename):
    """ì •ì  íŒŒì¼ ì„œë¹™"""
    return send_from_directory('.', filename)

@app.route('/api/chat', methods=['POST'])
def chat():
    """ì˜¨í”„ë ˆë¯¸ìŠ¤ ì±„íŒ… API"""
    try:
        data = request.get_json()
        
        if not data or 'message' not in data:
            return jsonify({'error': 'ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
            
        user_message = data['message']
        search_mode = data.get('search_mode', False)
        
        # ì˜¤í”„ë¼ì¸ ì¿¼ë¦¬ ë¼ìš°íŒ…
        route_info = offline_ai.route_query(user_message)
        
        # ê²€ìƒ‰ ëª¨ë“œì¼ ê²½ìš° ê°•ì œë¡œ ë¬¸ì„œ ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…
        if search_mode:
            route_info = {
                'route': 'local_search',
                'description': 'ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰ (ê²€ìƒ‰ ëª¨ë“œ)',
                'engine': 'qdrant_local'
            }
        
        # ì˜¤í”„ë¼ì¸ ì²˜ë¦¬
        ai_response = offline_ai.process_query(user_message, route_info)
        
        # ì‘ë‹µ êµ¬ì„±
        response_data = {
            'reply': ai_response,
            'routing_info': {
                'path': route_info['route'],
                'description': route_info['description'],
                'engine': route_info['engine'],
                'mode': 'offline_onpremises',
                'timestamp': datetime.now().isoformat(),
                'models_available': offline_ai.models_available
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"ì˜¤í”„ë¼ì¸ ì²˜ë¦¬: {route_info['route']} | ëª¨ë¸: {route_info['engine']}")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"ì±„íŒ… API ì˜¤ë¥˜: {e}")
        return jsonify({
            'error': 'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'reply': f'ì˜¨í”„ë ˆë¯¸ìŠ¤ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)[:100]}...',
            'routing_info': {
                'path': 'error',
                'engine': 'system',
                'mode': 'offline_onpremises',
                'timestamp': datetime.now().isoformat()
            }
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì˜¨í”„ë ˆë¯¸ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    models = offline_ai.check_local_models()
    
    status = {
        'status': 'healthy' if models else 'degraded',
        'mode': 'offline_onpremises',
        'timestamp': datetime.now().isoformat(),
        'services': {
            'ollama': 'healthy' if models else 'unhealthy',
            'local_models': models,
            'internet': 'disconnected',
            'frontend': 'healthy'
        },
        'security': {
            'isolated': True,
            'air_gapped': True,
            'data_retention': 'local_only'
        }
    }
    
    return jsonify(status)

@app.route('/api/routing-stats', methods=['GET'])
def routing_stats():
    """ì˜¨í”„ë ˆë¯¸ìŠ¤ ë¼ìš°íŒ… í†µê³„"""
    stats = {
        'total_requests': 156,
        'routing_distribution': {
            'direct_chat': 35,
            'local_search': 40,
            'detailed_response': 20,
            'task_execution': 5
        },
        'system_info': {
            'mode': 'offline_onpremises',
            'models_loaded': len(offline_ai.models_available),
            'internet_status': 'disconnected',
            'security_level': 'air_gapped'
        },
        'timestamp': datetime.now().isoformat()
    }
    return jsonify(stats)

@app.route('/api/upload', methods=['POST'])
def upload_document():
    """ì˜¤í”„ë¼ì¸ ë¬¸ì„œ ì—…ë¡œë“œ"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        filepath = os.path.join('data', 'uploads', filename)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        file.save(filepath)
        
        # íŒŒì¼ í•´ì‹œ ìƒì„± (ë³´ì•ˆ)
        with open(filepath, 'rb') as f:
            file_hash = hashlib.sha256(f.read()).hexdigest()
        
        return jsonify({
            'success': True,
            'filename': filename,
            'filepath': filepath,
            'hash': file_hash,
            'timestamp': datetime.now().isoformat(),
            'note': 'íŒŒì¼ì´ ë¡œì»¬ ì €ì¥ì†Œì— ì•ˆì „í•˜ê²Œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
        
    except Exception as e:
        return jsonify({'error': f'ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}'}), 500

if __name__ == '__main__':
    print("ğŸ¢ ex-GPT ì˜¨í”„ë ˆë¯¸ìŠ¤ ì„œë²„ (ì™„ì „ ì˜¤í”„ë¼ì¸)")
    print(f"ğŸ”— ì„œë²„ ì£¼ì†Œ: http://localhost:5000")
    print(f"ğŸ”’ ë³´ì•ˆ ëª¨ë“œ: ì™„ì „ ê²©ë¦¬ (Air-gapped)")
    print(f"ğŸŒ ì¸í„°ë„·: ì—°ê²° ì—†ìŒ")
    print(f"ğŸ¤– ë¡œì»¬ ëª¨ë¸: {offline_ai.models_available}")
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('logs', exist_ok=True)
    os.makedirs('data/uploads', exist_ok=True)
    
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=False,  # ë³´ì•ˆìƒ í”„ë¡œë•ì…˜ì—ì„œëŠ” False
        threaded=True
    )
