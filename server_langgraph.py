#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ex-GPT Enterprise Edition - LangGraph Based Architecture
ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AI ì–´ì‹œìŠ¤í„´íŠ¸ í”Œë«í¼

LangGraph ê¸°ë°˜ ì•„í‚¤í…ì²˜:
- ì‚¬ìš©ì ì§ˆì˜ â†’ LangGraph ë¼ìš°í„° â†’ direct_llm/rag_search/query_expansion/mcp_action
- ë©€í‹° ì—”ì§„ ì§€ì› (Ollama, RAGFlow, DSRAG)
- ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ì•ˆ & ëª¨ë‹ˆí„°ë§
"""

import os
import json
import uuid
import time
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Any, Literal
from dataclasses import dataclass
from enum import Enum

# Web Framework
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS

# LangGraph & LangChain
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI

# Ollama import
try:
    from langchain_ollama import OllamaLLM as Ollama
except ImportError:
    from langchain_community.llms import Ollama

# Utilities
import requests
from loguru import logger
from dotenv import load_dotenv

# Load environment
load_dotenv('.env.enterprise')

# Configuration
class Config:
    """ì—”í„°í”„ë¼ì´ì¦ˆ ì„¤ì •"""
    # Server
    HOST = os.getenv('HOST', '0.0.0.0')
    PORT = int(os.getenv('PORT', 5000))
    DEBUG = os.getenv('DEBUG', 'False').lower() == 'true'
    
    # LLM Engines
    OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
    
    # RAG Engines
    RAGFLOW_API_URL = os.getenv('RAGFLOW_API_URL', 'http://localhost:9380')
    RAGFLOW_API_KEY = os.getenv('RAGFLOW_API_KEY', '')
    QDRANT_URL = os.getenv('QDRANT_URL', 'http://localhost:6333')
    
    # Enterprise Features
    JWT_SECRET = os.getenv('JWT_SECRET', 'ex-gpt-enterprise-secret')
    REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')
    ENABLE_MONITORING = os.getenv('ENABLE_MONITORING', 'True').lower() == 'true'

# Data Models
@dataclass
class ChatState:
    """LangGraph ìƒíƒœ ê´€ë¦¬"""
    messages: List[dict]
    user_query: str
    route_decision: Optional[str] = None
    llm_response: Optional[str] = None
    rag_results: Optional[List[dict]] = None
    expanded_query: Optional[str] = None
    mcp_result: Optional[dict] = None
    routing_info: Optional[dict] = None
    
class RouteType(Enum):
    """ë¼ìš°íŒ… íƒ€ì…"""
    DIRECT_LLM = "direct_llm"        # 30%
    RAG_SEARCH = "rag_search"        # 50%
    QUERY_EXPANSION = "query_expansion"  # 15%
    MCP_ACTION = "mcp_action"        # 5%

# LangGraph Router
class ExGPTRouter:
    """ex-GPT LangGraph ë¼ìš°í„°"""
    
    def __init__(self):
        self.ollama_llm = None
        self.openai_llm = None
        self._init_llms()
        
    def _init_llms(self):
        """LLM ì´ˆê¸°í™”"""
        try:
            # Ollama ì´ˆê¸°í™”
            self.ollama_llm = Ollama(
                base_url=Config.OLLAMA_BASE_URL,
                model="qwen3:8b"
            )
            logger.info("Ollama LLM ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Ollama LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
        try:
            # OpenAI ì´ˆê¸°í™” (ë°±ì—…)
            if Config.OPENAI_API_KEY:
                self.openai_llm = ChatOpenAI(
                    api_key=Config.OPENAI_API_KEY,
                    model="gpt-3.5-turbo"
                )
                logger.info("OpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"OpenAI LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def classify_query(self, state: ChatState) -> ChatState:
        """ì§ˆì˜ ë¶„ë¥˜ (LangGraph ë…¸ë“œ)"""
        query = state.user_query.lower()
        
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ (í–¥í›„ ML ëª¨ë¸ë¡œ êµì²´ ê°€ëŠ¥)
        if any(keyword in query for keyword in ['ê²€ìƒ‰', 'ì°¾ì•„', 'ë¬¸ì„œ', 'ìë£Œ']):
            state.route_decision = RouteType.RAG_SEARCH.value
        elif any(keyword in query for keyword in ['ìì„¸íˆ', 'ìƒì„¸íˆ', 'êµ¬ì²´ì ìœ¼ë¡œ']):
            state.route_decision = RouteType.QUERY_EXPANSION.value
        elif any(keyword in query for keyword in ['ì‹¤í–‰', 'ì‘ì—…', 'ì²˜ë¦¬']):
            state.route_decision = RouteType.MCP_ACTION.value
        else:
            state.route_decision = RouteType.DIRECT_LLM.value
            
        state.routing_info = {
            "path": state.route_decision,
            "timestamp": datetime.now().isoformat(),
            "query_length": len(state.user_query)
        }
        
        logger.info(f"Query classified as: {state.route_decision}")
        return state
    
    def direct_llm_response(self, state: ChatState) -> ChatState:
        """Direct LLM ì‘ë‹µ (30%)"""
        try:
            # Ollama ìš°ì„  ì‹œë„
            if self.ollama_llm:
                response = self.ollama_llm.invoke(state.user_query)
                state.llm_response = response
                state.routing_info["engine"] = "ollama"
            elif self.openai_llm:
                messages = [HumanMessage(content=state.user_query)]
                response = self.openai_llm.invoke(messages)
                state.llm_response = response.content
                state.routing_info["engine"] = "openai"
            else:
                state.llm_response = "ì£„ì†¡í•©ë‹ˆë‹¤. LLM ì„œë¹„ìŠ¤ê°€ í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                state.routing_info["engine"] = "fallback"
                
        except Exception as e:
            logger.error(f"Direct LLM ì˜¤ë¥˜: {e}")
            state.llm_response = f"ì¼ë°˜ ìƒì‹ ì§ˆë¬¸: {state.user_query}\n\nì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ LLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            state.routing_info["engine"] = "error"
            
        return state
    
    def rag_search_response(self, state: ChatState) -> ChatState:
        """RAG ê²€ìƒ‰ ì‘ë‹µ (50%)"""
        try:
            # RAGFlow ë˜ëŠ” DSRAG í˜¸ì¶œ
            rag_engine = getattr(state, 'rag_engine', 'ragflow')
            
            if rag_engine == 'ragflow':
                state = self._call_ragflow(state)
            else:
                state = self._call_dsrag(state)
                
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            state.llm_response = f"ë¬¸ì„œ ê²€ìƒ‰ ì§ˆë¬¸: {state.user_query}\n\nì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì„œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            state.routing_info["engine"] = "error"
            
        return state
    
    def _call_ragflow(self, state: ChatState) -> ChatState:
        """RAGFlow í˜¸ì¶œ"""
        try:
            # RAGFlow API í˜¸ì¶œ
            response = requests.post(
                f"{Config.RAGFLOW_API_URL}/api/chat",
                json={
                    "query": state.user_query,
                    "top_k": 5
                },
                headers={
                    "Authorization": f"Bearer {Config.RAGFLOW_API_KEY}"
                },
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                state.llm_response = data.get('answer', 'ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')
                state.rag_results = data.get('references', [])
                state.routing_info["engine"] = "ragflow"
            else:
                raise Exception(f"RAGFlow API ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            logger.error(f"RAGFlow í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            # Fallback to direct LLM
            state = self.direct_llm_response(state)
            state.routing_info["engine"] = "ragflow_fallback"
            
        return state
    
    def _call_dsrag(self, state: ChatState) -> ChatState:
        """DSRAG í˜¸ì¶œ"""
        try:
            # DSRAG ë¡œì§ (Qdrant + ë¡œì»¬ ì„ë² ë”©)
            state.llm_response = f"DSRAG ê²€ìƒ‰ ê²°ê³¼: {state.user_query}\n\n[êµ¬í˜„ ì˜ˆì • - Qdrant ë²¡í„° ê²€ìƒ‰ + ë¡œì»¬ LLM]"
            state.routing_info["engine"] = "dsrag"
            
        except Exception as e:
            logger.error(f"DSRAG í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            # Fallback to direct LLM
            state = self.direct_llm_response(state)
            state.routing_info["engine"] = "dsrag_fallback"
            
        return state
    
    def query_expansion_response(self, state: ChatState) -> ChatState:
        """ì§ˆë¬¸ í™•ì¥ ì‘ë‹µ (15%)"""
        try:
            # ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í™•ì¥
            expanded_query = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: {state.user_query}"
            
            # í™•ì¥ëœ ì§ˆë¬¸ìœ¼ë¡œ LLM í˜¸ì¶œ
            temp_state = ChatState(
                messages=state.messages,
                user_query=expanded_query
            )
            temp_state = self.direct_llm_response(temp_state)
            
            state.expanded_query = expanded_query
            state.llm_response = temp_state.llm_response
            state.routing_info["engine"] = "query_expansion"
            
        except Exception as e:
            logger.error(f"Query expansion ì˜¤ë¥˜: {e}")
            state = self.direct_llm_response(state)
            state.routing_info["engine"] = "expansion_fallback"
            
        return state
    
    def mcp_action_response(self, state: ChatState) -> ChatState:
        """MCP ì•¡ì…˜ ì‘ë‹µ (5%)"""
        try:
            # MCP (Model Context Protocol) ì•¡ì…˜
            state.mcp_result = {
                "action": "task_automation",
                "query": state.user_query,
                "status": "planned"
            }
            state.llm_response = f"ì‘ì—… ìë™í™” ìš”ì²­: {state.user_query}\n\n[êµ¬í˜„ ì˜ˆì • - MCP ê¸°ë°˜ ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™]"
            state.routing_info["engine"] = "mcp"
            
        except Exception as e:
            logger.error(f"MCP ì•¡ì…˜ ì˜¤ë¥˜: {e}")
            state = self.direct_llm_response(state)
            state.routing_info["engine"] = "mcp_fallback"
            
        return state
    
    def create_graph(self):
        """LangGraph ìƒì„±"""
        workflow = StateGraph(ChatState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("classify", self.classify_query)
        workflow.add_node("direct_llm", self.direct_llm_response)
        workflow.add_node("rag_search", self.rag_search_response)
        workflow.add_node("query_expansion", self.query_expansion_response)
        workflow.add_node("mcp_task", self.mcp_action_response)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge(START, "classify")
        
        # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "classify",
            lambda state: state.route_decision,
            {
                RouteType.DIRECT_LLM.value: "direct_llm",
                RouteType.RAG_SEARCH.value: "rag_search",
                RouteType.QUERY_EXPANSION.value: "query_expansion",
                RouteType.MCP_ACTION.value: "mcp_task"
            }
        )
        
        # ëª¨ë“  ë…¸ë“œì—ì„œ ENDë¡œ
        workflow.add_edge("direct_llm", END)
        workflow.add_edge("rag_search", END)
        workflow.add_edge("query_expansion", END)
        workflow.add_edge("mcp_task", END)
        
        return workflow.compile()

# Flask App
app = Flask(__name__)
CORS(app)

# LangGraph ë¼ìš°í„° ì´ˆê¸°í™”
router = ExGPTRouter()
graph = router.create_graph()

# Logging ì„¤ì •
logger.add("logs/ex-gpt-enterprise.log", rotation="1 day", retention="30 days")

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return send_from_directory('.', 'index.html')

@app.route('/<path:filename>')
def static_files(filename):
    """ì •ì  íŒŒì¼ ì„œë¹™"""
    return send_from_directory('.', filename)

@app.route('/api/chat', methods=['POST'])
def chat():
    """ì±„íŒ… API - LangGraph ê¸°ë°˜"""
    try:
        data = request.get_json()
        
        if not data or 'message' not in data:
            return jsonify({'error': 'ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
            
        user_message = data['message']
        search_mode = data.get('search_mode', False)
        rag_engine = data.get('rag_engine', 'ragflow')
        conversation_history = data.get('conversation_history', [])
        
        # ì±„íŒ… ìƒíƒœ ìƒì„±
        state = ChatState(
            messages=conversation_history,
            user_query=user_message
        )
        
        # RAG ì—”ì§„ ì„¤ì •
        state.rag_engine = rag_engine
        
        # ê²€ìƒ‰ ëª¨ë“œì¼ ê²½ìš° RAG ìš°ì„ 
        if search_mode:
            state.route_decision = RouteType.RAG_SEARCH.value
        
        # LangGraph ì‹¤í–‰
        logger.info(f"Processing query: {user_message[:100]}...")
        result = graph.invoke(state)
        
        # ì‘ë‹µ êµ¬ì„±
        response_data = {
            'reply': result.llm_response,
            'routing_info': result.routing_info,
            'rag_results': result.rag_results,
            'expanded_query': result.expanded_query,
            'mcp_result': result.mcp_result,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"Response generated via: {result.routing_info.get('path', 'unknown')}")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Chat API ì˜¤ë¥˜: {e}")
        return jsonify({
            'error': 'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'reply': 'ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
            'routing_info': {
                'path': 'error',
                'engine': 'system',
                'timestamp': datetime.now().isoformat()
            }
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    status = {
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'services': {
            'ollama': 'unknown',
            'ragflow': 'unknown',
            'qdrant': 'unknown'
        }
    }
    
    # Ollama ìƒíƒœ í™•ì¸
    try:
        response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)
        status['services']['ollama'] = 'healthy' if response.status_code == 200 else 'unhealthy'
    except:
        status['services']['ollama'] = 'unhealthy'
    
    # RAGFlow ìƒíƒœ í™•ì¸
    try:
        response = requests.get(f"{Config.RAGFLOW_API_URL}/api/health", timeout=5)
        status['services']['ragflow'] = 'healthy' if response.status_code == 200 else 'unhealthy'
    except:
        status['services']['ragflow'] = 'unhealthy'
    
    # Qdrant ìƒíƒœ í™•ì¸
    try:
        response = requests.get(f"{Config.QDRANT_URL}/collections", timeout=5)
        status['services']['qdrant'] = 'healthy' if response.status_code == 200 else 'unhealthy'
    except:
        status['services']['qdrant'] = 'unhealthy'
    
    return jsonify(status)

@app.route('/api/routing-stats', methods=['GET'])
def routing_stats():
    """ë¼ìš°íŒ… í†µê³„"""
    # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” Redisë‚˜ DBì—ì„œ í†µê³„ ì¡°íšŒ
    stats = {
        'total_requests': 1000,
        'routing_distribution': {
            'direct_llm': 30,
            'rag_search': 50,
            'query_expansion': 15,
            'mcp_action': 5
        },
        'engine_usage': {
            'ollama': 70,
            'ragflow': 25,
            'openai': 5
        },
        'timestamp': datetime.now().isoformat()
    }
    return jsonify(stats)

if __name__ == '__main__':
    logger.info("ğŸš€ ex-GPT Enterprise Edition ì‹œì‘")
    logger.info(f"ğŸ”— ì„œë²„ ì£¼ì†Œ: http://{Config.HOST}:{Config.PORT}")
    logger.info(f"ğŸ¤– Ollama: {Config.OLLAMA_BASE_URL}")
    logger.info(f"ğŸ“š RAGFlow: {Config.RAGFLOW_API_URL}")
    logger.info(f"ğŸ” Qdrant: {Config.QDRANT_URL}")
    
    app.run(
        host=Config.HOST,
        port=Config.PORT,
        debug=Config.DEBUG,
        threaded=True
    )
