#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
EX-GPT ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ
- ë¬¸ì„œ íŒŒì‹± (PDF, HWP, Excel, Word)
- Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
- ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›
- ê´€ë¦¬ì ì›¹ ì¸í„°í˜ì´ìŠ¤
"""

import os
import hashlib
import json
import logging
from datetime import datetime
from pathlib import Path
import uuid
import mimetypes

# ë¬¸ì„œ íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬
import PyPDF2
import fitz  # PyMuPDF (ë” ì¢‹ì€ PDF íŒŒì‹±)
import docx
import openpyxl
import pandas as pd
import olefile  # HWP íŒŒì¼ìš©
import mammoth  # Word ë¬¸ì„œ ê³ ê¸‰ íŒŒì‹±

# ë²¡í„° DB ë° ì„ë² ë”©
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct
import sentence_transformers
from sentence_transformers import SentenceTransformer

# í…ìŠ¤íŠ¸ ì²˜ë¦¬
import re
from typing import List, Dict, Optional, Tuple
import asyncio
from concurrent.futures import ThreadPoolExecutor

# Flask ê´€ë ¨
from flask import Blueprint, request, jsonify, render_template_string, send_from_directory
from werkzeug.utils import secure_filename

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentParser:
    """ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ íŒŒì‹± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.supported_formats = {
            '.pdf': self.parse_pdf,
            '.docx': self.parse_docx,
            '.doc': self.parse_doc,
            '.xlsx': self.parse_excel,
            '.xls': self.parse_excel,
            '.hwp': self.parse_hwp,
            '.txt': self.parse_txt
        }
    
    def parse_pdf(self, file_path: str) -> List[Dict]:
        """PDF íŒŒì¼ íŒŒì‹±"""
        chunks = []
        try:
            # PyMuPDF ì‚¬ìš© (ë” ì •í™•í•œ íŒŒì‹±)
            doc = fitz.open(file_path)
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = page.get_text()
                
                if text.strip():
                    chunks.append({
                        'content': text.strip(),
                        'page': page_num + 1,
                        'type': 'pdf_page'
                    })
            doc.close()
            
        except Exception as e:
            logger.error(f"PDF íŒŒì‹± ì˜¤ë¥˜ ({file_path}): {e}")
            # í´ë°±: PyPDF2 ì‚¬ìš©
            try:
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for page_num, page in enumerate(pdf_reader.pages):
                        text = page.extract_text()
                        if text.strip():
                            chunks.append({
                                'content': text.strip(),
                                'page': page_num + 1,
                                'type': 'pdf_page'
                            })
            except Exception as e2:
                logger.error(f"PDF í´ë°± íŒŒì‹±ë„ ì‹¤íŒ¨: {e2}")
        
        return chunks
    
    def parse_docx(self, file_path: str) -> List[Dict]:
        """DOCX íŒŒì¼ íŒŒì‹±"""
        chunks = []
        try:
            # mammoth ì‚¬ìš© (ë” ì •í™•í•œ íŒŒì‹±)
            with open(file_path, "rb") as docx_file:
                result = mammoth.extract_raw_text(docx_file)
                text = result.value
                
                if text.strip():
                    # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• 
                    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
                    for i, para in enumerate(paragraphs):
                        if len(para) > 50:  # ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ë§Œ
                            chunks.append({
                                'content': para,
                                'paragraph': i + 1,
                                'type': 'docx_paragraph'
                            })
                
        except Exception as e:
            logger.error(f"DOCX mammoth íŒŒì‹± ì˜¤ë¥˜, docx ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„: {e}")
            # í´ë°±: python-docx ì‚¬ìš©
            try:
                doc = docx.Document(file_path)
                for i, para in enumerate(doc.paragraphs):
                    if para.text.strip() and len(para.text.strip()) > 50:
                        chunks.append({
                            'content': para.text.strip(),
                            'paragraph': i + 1,
                            'type': 'docx_paragraph'
                        })
            except Exception as e2:
                logger.error(f"DOCX í´ë°± íŒŒì‹±ë„ ì‹¤íŒ¨: {e2}")
        
        return chunks
    
    def parse_doc(self, file_path: str) -> List[Dict]:
        """DOC íŒŒì¼ íŒŒì‹± (ì œí•œì )"""
        # DOC íŒŒì¼ì€ ë³µì¡í•˜ë¯€ë¡œ ê¸°ë³¸ì ì¸ ì²˜ë¦¬ë§Œ
        logger.warning(f"DOC íŒŒì¼ì€ ì œí•œì ìœ¼ë¡œ ì§€ì›ë©ë‹ˆë‹¤: {file_path}")
        return [{'content': f'DOC íŒŒì¼ {file_path}ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•í•œ íŒŒì‹±ì„ ìœ„í•´ DOCXë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.', 'type': 'doc_notice'}]
    
    def parse_excel(self, file_path: str) -> List[Dict]:
        """Excel íŒŒì¼ íŒŒì‹±"""
        chunks = []
        try:
            # openpyxlê³¼ pandas ë‘˜ ë‹¤ ì‚¬ìš©
            workbook = openpyxl.load_workbook(file_path)
            
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                
                # ë°ì´í„° ë²”ìœ„ ì°¾ê¸°
                max_row = sheet.max_row
                max_col = sheet.max_column
                
                if max_row > 1 and max_col > 1:
                    # í—¤ë”ì™€ ë°ì´í„° ì¶”ì¶œ
                    headers = []
                    for col in range(1, max_col + 1):
                        header = sheet.cell(row=1, column=col).value
                        headers.append(str(header) if header else f"Column_{col}")
                    
                    # ê° í–‰ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    for row in range(2, min(max_row + 1, 1000)):  # ìµœëŒ€ 1000í–‰
                        row_data = []
                        for col in range(1, max_col + 1):
                            cell_value = sheet.cell(row=row, column=col).value
                            row_data.append(str(cell_value) if cell_value else "")
                        
                        if any(row_data):  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°
                            content = " | ".join([f"{h}: {v}" for h, v in zip(headers, row_data) if v])
                            chunks.append({
                                'content': content,
                                'sheet': sheet_name,
                                'row': row,
                                'type': 'excel_row'
                            })
            
            workbook.close()
            
        except Exception as e:
            logger.error(f"Excel íŒŒì‹± ì˜¤ë¥˜: {e}")
            # í´ë°±: pandas ì‚¬ìš©
            try:
                excel_file = pd.ExcelFile(file_path)
                for sheet_name in excel_file.sheet_names:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    for index, row in df.iterrows():
                        content = " | ".join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
                        if content:
                            chunks.append({
                                'content': content,
                                'sheet': sheet_name,
                                'row': index + 2,
                                'type': 'excel_row'
                            })
            except Exception as e2:
                logger.error(f"Excel í´ë°± íŒŒì‹±ë„ ì‹¤íŒ¨: {e2}")
        
        return chunks
    
    def parse_hwp(self, file_path: str) -> List[Dict]:
        """HWP íŒŒì¼ íŒŒì‹±"""
        chunks = []
        try:
            # olefileì„ ì‚¬ìš©í•œ ê¸°ë³¸ì ì¸ HWP íŒŒì‹±
            f = olefile.OleFileIO(file_path)
            
            # HWP íŒŒì¼ êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            if f.exists('PrvText'):
                encoded_text = f.openstream('PrvText').read()
                try:
                    # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
                    for encoding in ['utf-16', 'cp949', 'euc-kr']:
                        try:
                            text = encoded_text.decode(encoding)
                            if text and len(text) > 10:
                                # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• 
                                paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
                                for i, para in enumerate(paragraphs):
                                    if len(para) > 20:
                                        chunks.append({
                                            'content': para,
                                            'paragraph': i + 1,
                                            'type': 'hwp_paragraph'
                                        })
                                break
                        except UnicodeDecodeError:
                            continue
                except Exception as e:
                    logger.error(f"HWP í…ìŠ¤íŠ¸ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            
            f.close()
            
        except Exception as e:
            logger.error(f"HWP íŒŒì‹± ì˜¤ë¥˜: {e}")
            chunks.append({
                'content': f'HWP íŒŒì¼ {os.path.basename(file_path)}ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•í•œ íŒŒì‹±ì„ ìœ„í•´ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ì„ ê¶Œì¥í•©ë‹ˆë‹¤.',
                'type': 'hwp_notice'
            })
        
        return chunks
    
    def parse_txt(self, file_path: str) -> List[Dict]:
        """TXT íŒŒì¼ íŒŒì‹±"""
        chunks = []
        try:
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
            for encoding in ['utf-8', 'cp949', 'euc-kr', 'latin-1']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    
                    if content.strip():
                        # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• 
                        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
                        for i, para in enumerate(paragraphs):
                            if len(para) > 20:
                                chunks.append({
                                    'content': para,
                                    'paragraph': i + 1,
                                    'type': 'txt_paragraph'
                                })
                    break
                    
                except UnicodeDecodeError:
                    continue
                    
        except Exception as e:
            logger.error(f"TXT íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return chunks
    
    def parse_file(self, file_path: str) -> List[Dict]:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ íŒŒì„œ ì„ íƒ"""
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in self.supported_formats:
            logger.info(f"íŒŒì‹± ì‹œì‘: {file_path} ({file_ext})")
            chunks = self.supported_formats[file_ext](file_path)
            logger.info(f"íŒŒì‹± ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return chunks
        else:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
            return []

class KnowledgeManager:
    """ì§€ì‹ ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, qdrant_host="localhost", qdrant_port=6333, collection_name="documents"):
        self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.collection_name = collection_name
        self.parser = DocumentParser()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.vector_size = 384  # MiniLM-L12-v2ì˜ ë²¡í„° í¬ê¸°
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥ íŒŒì¼
        self.metadata_file = "knowledge_metadata.json"
        self.file_hashes = self.load_metadata()
        
        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.ensure_collection_exists()
    
    def ensure_collection_exists(self):
        """Qdrant ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        try:
            # ì»¬ë ‰ì…˜ ëª©ë¡ í™•ì¸
            collections = self.qdrant_client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.collection_name not in collection_names:
                logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì¤‘...")
                
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.vector_size,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            else:
                logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ì¡´ì¬ í™•ì¸")
                
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ í™•ì¸/ìƒì„± ì˜¤ë¥˜: {e}")
            raise
    
    def load_metadata(self) -> Dict:
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        try:
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {}
    
    def save_metadata(self):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_hashes, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def get_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ì˜ MD5 í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
        except Exception as e:
            logger.error(f"íŒŒì¼ í•´ì‹œ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return ""
        return hash_md5.hexdigest()
    
    def is_file_updated(self, file_path: str) -> bool:
        """íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        current_hash = self.get_file_hash(file_path)
        filename = os.path.basename(file_path)
        
        if filename in self.file_hashes:
            return self.file_hashes[filename]['hash'] != current_hash
        return True  # ìƒˆ íŒŒì¼
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ëª©ë¡ì„ ë²¡í„°ë¡œ ì„ë² ë”©"""
        try:
            embeddings = self.embedding_model.encode(texts)
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return [[0.0] * self.vector_size] * len(texts)
    
    def process_file(self, file_path: str, force_update: bool = False) -> Dict:
        """íŒŒì¼ ì²˜ë¦¬ ë° ë²¡í„° DB ì €ì¥"""
        filename = os.path.basename(file_path)
        
        # íŒŒì¼ ì—…ë°ì´íŠ¸ í™•ì¸
        if not force_update and not self.is_file_updated(file_path):
            logger.info(f"íŒŒì¼ '{filename}'ì€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
            return {
                'success': True,
                'message': f'íŒŒì¼ {filename}ì€ ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.',
                'chunks_added': 0,
                'skipped': True
            }
        
        try:
            # ê¸°ì¡´ íŒŒì¼ ë°ì´í„° ì‚­ì œ (ì¦ë¶„ ì—…ë°ì´íŠ¸)
            self.delete_file_from_db(filename)
            
            # íŒŒì¼ íŒŒì‹±
            chunks = self.parser.parse_file(file_path)
            
            if not chunks:
                return {
                    'success': False,
                    'message': f'íŒŒì¼ {filename}ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'chunks_added': 0
                }
            
            # ì„ë² ë”© ìƒì„±
            texts = [chunk['content'] for chunk in chunks]
            embeddings = self.embed_texts(texts)
            
            # Qdrantì— ì €ì¥í•  í¬ì¸íŠ¸ ìƒì„±
            points = []
            current_time = datetime.now().isoformat()
            file_hash = self.get_file_hash(file_path)
            
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                point = PointStruct(
                    id=str(uuid.uuid4()),
                    vector=embedding,
                    payload={
                        'filename': filename,
                        'content': chunk['content'],
                        'file_hash': file_hash,
                        'chunk_id': i,
                        'total_chunks': len(chunks),
                        'chunk_type': chunk.get('type', 'unknown'),
                        'page': chunk.get('page'),
                        'paragraph': chunk.get('paragraph'),
                        'sheet': chunk.get('sheet'),
                        'row': chunk.get('row'),
                        'upload_time': current_time,
                        'file_size': os.path.getsize(file_path),
                        'file_ext': Path(file_path).suffix.lower()
                    }
                )
                points.append(point)
            
            # Qdrantì— ì—…ì„œíŠ¸
            self.qdrant_client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.file_hashes[filename] = {
                'hash': file_hash,
                'upload_time': current_time,
                'chunks': len(chunks),
                'file_size': os.path.getsize(file_path),
                'file_ext': Path(file_path).suffix.lower()
            }
            self.save_metadata()
            
            logger.info(f"íŒŒì¼ '{filename}' ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            
            return {
                'success': True,
                'message': f'íŒŒì¼ {filename}ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'chunks_added': len(chunks),
                'skipped': False
            }
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({filename}): {e}")
            return {
                'success': False,
                'message': f'íŒŒì¼ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'chunks_added': 0
            }
    
    def delete_file_from_db(self, filename: str):
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ DBì—ì„œ ì‚­ì œ"""
        try:
            # íŒŒì¼ëª…ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ì‚­ì œ
            self.qdrant_client.delete(
                collection_name=self.collection_name,
                points_selector={
                    "filter": {
                        "must": [
                            {
                                "key": "filename",
                                "match": {"value": filename}
                            }
                        ]
                    }
                }
            )
            logger.info(f"íŒŒì¼ '{filename}'ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì˜¤ë¥˜ ({filename}): {e}")
    
    def search_documents(self, query: str, limit: int = 5) -> List[Dict]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_vector = self.embedding_model.encode([query])[0].tolist()
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                limit=limit,
                with_payload=True
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for result in search_results:
                results.append({
                    'filename': result.payload.get('filename', 'Unknown'),
                    'content': result.payload.get('content', ''),
                    'score': float(result.score),
                    'page': result.payload.get('page'),
                    'paragraph': result.payload.get('paragraph'),
                    'chunk_type': result.payload.get('chunk_type'),
                    'upload_time': result.payload.get('upload_time')
                })
            
            return results
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_collection_stats(self) -> Dict:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´"""
        try:
            info = self.qdrant_client.get_collection(self.collection_name)
            
            # íŒŒì¼ë³„ í†µê³„
            file_stats = {}
            for filename, metadata in self.file_hashes.items():
                file_stats[filename] = {
                    'chunks': metadata['chunks'],
                    'upload_time': metadata['upload_time'],
                    'file_size': metadata['file_size'],
                    'file_ext': metadata['file_ext']
                }
            
            return {
                'total_vectors': info.vectors_count,
                'total_files': len(self.file_hashes),
                'file_details': file_stats,
                'collection_name': self.collection_name
            }
            
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {}
    
    def process_directory(self, directory_path: str, force_update: bool = False) -> Dict:
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ì§€ì› íŒŒì¼ ì²˜ë¦¬"""
        results = {
            'total_files': 0,
            'processed_files': 0,
            'skipped_files': 0,
            'failed_files': 0,
            'total_chunks': 0,
            'details': []
        }
        
        supported_extensions = {'.pdf', '.docx', '.doc', '.xlsx', '.xls', '.hwp', '.txt'}
        
        try:
            for root, dirs, files in os.walk(directory_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    file_ext = Path(file_path).suffix.lower()
                    
                    if file_ext in supported_extensions:
                        results['total_files'] += 1
                        
                        result = self.process_file(file_path, force_update)
                        results['details'].append({
                            'filename': file,
                            'result': result
                        })
                        
                        if result['success']:
                            if result.get('skipped', False):
                                results['skipped_files'] += 1
                            else:
                                results['processed_files'] += 1
                                results['total_chunks'] += result['chunks_added']
                        else:
                            results['failed_files'] += 1
            
            logger.info(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: {results['processed_files']}/{results['total_files']} íŒŒì¼")
            
        except Exception as e:
            logger.error(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            results['error'] = str(e)
        
        return results

# Flask Blueprint ìƒì„±
knowledge_bp = Blueprint('knowledge', __name__)

# ì „ì—­ KnowledgeManager ì¸ìŠ¤í„´ìŠ¤
knowledge_manager = None

def init_knowledge_manager():
    """KnowledgeManager ì´ˆê¸°í™”"""
    global knowledge_manager
    try:
        knowledge_manager = KnowledgeManager()
        logger.info("Knowledge Manager ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"Knowledge Manager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

@knowledge_bp.route('/api/knowledge/upload', methods=['POST'])
def upload_document():
    """ë¬¸ì„œ ì—…ë¡œë“œ API"""
    if not knowledge_manager:
        return jsonify({'error': 'Knowledge Managerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 500
    
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        # íŒŒì¼ ì €ì¥
        filename = secure_filename(file.filename)
        upload_dir = 'uploads'
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, filename)
        file.save(file_path)
        
        # íŒŒì¼ ì²˜ë¦¬
        force_update = request.form.get('force_update', 'false').lower() == 'true'
        result = knowledge_manager.process_file(file_path, force_update)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(file_path)
        except:
            pass
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}'}), 500

@knowledge_bp.route('/api/knowledge/stats', methods=['GET'])
def get_knowledge_stats():
    """ì§€ì‹ ë² ì´ìŠ¤ í†µê³„"""
    if not knowledge_manager:
        return jsonify({'error': 'Knowledge Managerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 500
    
    try:
        stats = knowledge_manager.get_collection_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({'error': str(e)}), 500

@knowledge_bp.route('/api/knowledge/search', methods=['POST'])
def search_knowledge():
    """ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰"""
    if not knowledge_manager:
        return jsonify({'error': 'Knowledge Managerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 500
    
    try:
        data = request.json
        query = data.get('query', '')
        limit = data.get('limit', 5)
        
        if not query:
            return jsonify({'error': 'ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        results = knowledge_manager.search_documents(query, limit)
        return jsonify({'results': results})
        
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return jsonify({'error': str(e)}), 500

@knowledge_bp.route('/api/knowledge/process_directory', methods=['POST'])
def process_directory():
    """ë””ë ‰í† ë¦¬ ì¼ê´„ ì²˜ë¦¬"""
    if not knowledge_manager:
        return jsonify({'error': 'Knowledge Managerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 500
    
    try:
        data = request.json
        directory_path = data.get('directory_path', '')
        force_update = data.get('force_update', False)
        
        if not directory_path or not os.path.exists(directory_path):
            return jsonify({'error': 'ìœ íš¨í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        results = knowledge_manager.process_directory(directory_path, force_update)
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return jsonify({'error': str(e)}), 500

@knowledge_bp.route('/admin/knowledge')
def admin_knowledge():
    """ì§€ì‹ ê´€ë¦¬ ê´€ë¦¬ì í˜ì´ì§€"""
    # ê°„ë‹¨í•œ HTML í˜ì´ì§€ ë°˜í™˜
    html_content = """
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <title>EX-GPT ì§€ì‹ ê´€ë¦¬</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            .card { background: white; padding: 20px; margin: 10px; border: 1px solid #ddd; }
            .btn { padding: 10px 20px; margin: 5px; border: none; cursor: pointer; }
            .btn-primary { background: #007bff; color: white; }
        </style>
    </head>
    <body>
        <h1>EX-GPT ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ</h1>
        <div class="card">
            <h2>íŒŒì¼ ì—…ë¡œë“œ</h2>
            <input type="file" id="fileInput" multiple>
            <button class="btn btn-primary" onclick="uploadFiles()">ì—…ë¡œë“œ</button>
        </div>
        <div class="card">
            <h2>ê²€ìƒ‰ í…ŒìŠ¤íŠ¸</h2>
            <input type="text" id="searchInput" placeholder="ê²€ìƒ‰ì–´ ì…ë ¥">
            <button class="btn btn-primary" onclick="searchDocuments()">ê²€ìƒ‰</button>
            <div id="searchResults"></div>
        </div>
        
        <script>
            async function uploadFiles() {
                const fileInput = document.getElementById('fileInput');
                const files = fileInput.files;
                
                for (let file of files) {
                    const formData = new FormData();
                    formData.append('file', file);
                    
                    try {
                        const response = await fetch('/api/knowledge/upload', {
                            method: 'POST',
                            body: formData
                        });
                        const result = await response.json();
                        console.log('Upload result:', result);
                        alert(`${file.name}: ${result.message}`);
                    } catch (error) {
                        console.error('Upload error:', error);
                        alert(`${file.name}: ì—…ë¡œë“œ ì‹¤íŒ¨`);
                    }
                }
            }
            
            async function searchDocuments() {
                const query = document.getElementById('searchInput').value;
                
                try {
                    const response = await fetch('/api/knowledge/search', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ query: query, limit: 5 })
                    });
                    const result = await response.json();
                    
                    const resultsDiv = document.getElementById('searchResults');
                    resultsDiv.innerHTML = '<h3>ê²€ìƒ‰ ê²°ê³¼:</h3>';
                    
                    result.results.forEach((item, index) => {
                        resultsDiv.innerHTML += `
                            <div style="border: 1px solid #ccc; padding: 10px; margin: 5px;">
                                <h4>${item.filename} (ì ìˆ˜: ${item.score.toFixed(3)})</h4>
                                <p>${item.content.substring(0, 200)}...</p>
                            </div>
                        `;
                    });
                } catch (error) {
                    console.error('Search error:', error);
                }
            }
        </script>
    </body>
    </html>
    """
    return html_content

# ì‚¬ìš© ì˜ˆì‹œ ë° CLI ë„êµ¬
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='EX-GPT ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ')
    parser.add_argument('--init', action='store_true', help='Qdrant ì´ˆê¸°í™”')
    parser.add_argument('--upload', type=str, help='íŒŒì¼ ì—…ë¡œë“œ')
    parser.add_argument('--process-dir', type=str, help='ë””ë ‰í† ë¦¬ ì¼ê´„ ì²˜ë¦¬')
    parser.add_argument('--search', type=str, help='ê²€ìƒ‰ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--stats', action='store_true', help='í†µê³„ ë³´ê¸°')
    parser.add_argument('--force', action='store_true', help='ê°•ì œ ì—…ë°ì´íŠ¸')
    
    args = parser.parse_args()
    
    # Knowledge Manager ì´ˆê¸°í™”
    try:
        km = KnowledgeManager()
        print("âœ… Knowledge Manager ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        exit(1)
    
    if args.init:
        print("âœ… Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    elif args.upload:
        result = km.process_file(args.upload, args.force)
        print(f"ì—…ë¡œë“œ ê²°ê³¼: {result}")
    
    elif args.process_dir:
        result = km.process_directory(args.process_dir, args.force)
        print(f"ì¼ê´„ ì²˜ë¦¬ ê²°ê³¼: {result}")
    
    elif args.search:
        results = km.search_documents(args.search)
        print(f"ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['filename']} (ìœ ì‚¬ë„: {result['score']:.3f})")
            print(f"   ë‚´ìš©: {result['content'][:100]}...")
    
    elif args.stats:
        stats = km.get_collection_stats()
        print("ğŸ“Š ì§€ì‹ ë² ì´ìŠ¤ í†µê³„:")
        print(f"  ì´ ë²¡í„° ìˆ˜: {stats.get('total_vectors', 0)}")
        print(f"  ì´ íŒŒì¼ ìˆ˜: {stats.get('total_files', 0)}")
        print("  íŒŒì¼ ëª©ë¡:")
        for filename, details in stats.get('file_details', {}).items():
            print(f"    - {filename}: {details['chunks']}ê°œ ì²­í¬, {details['file_size']} bytes")
    
    else:
        parser.print_help()