#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from flask import Flask, request, jsonify, send_from_directory, render_template_string
from flask_cors import CORS
import torch
import torch.nn as nn
import os
import gc
import logging
import threading
import time
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import asyncio
from functools import wraps
import warnings
import whisper
import tempfile
from werkzeug.utils import secure_filename
import requests
import json
import librosa
import noisereduce as nr
from pydub import AudioSegment
from pydub.silence import split_on_silence
import numpy as np
from datetime import datetime, timedelta
import uuid
import sentence_transformers
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import uuid
import asyncio
from concurrent.futures import ThreadPoolExecutor
import queue
import threading
from tqdm import tqdm
import json
import zipfile
import os
import tempfile
from pathlib import Path
import subprocess
import psutil
import threading
from queue import Queue, Empty
import asyncio
from concurrent.futures import ThreadPoolExecutor
from flask import Response

# ============= Flask ì•± ë° CORS ì„¤ì • =============
app = Flask(__name__)
CORS(app)

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ë³€ìˆ˜ - ë“€ì–¼ GPU ì§€ì›
florence_models = {}
florence_processors = {}
available_devices = ["cuda:0", "cuda:1"]
request_queue = Queue()
gpu_load_tracker = {device: 0 for device in available_devices}
gpu_lock = threading.Lock()

# í†µê³„ ë°ì´í„° ì €ì¥
stats_data = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'active_users': set(),
    'daily_usage': {},
    'gpu_usage_history': [],
    'recent_logs': []
}

# ì—…ë¡œë“œ ì„¸ì…˜ ê´€ë¦¬
upload_sessions = {}

# ê¸€ë¡œë²Œ ë³€ìˆ˜ ì„ ì–¸
qdrant_client = None
embedding_model = None
whisper_model = None
optimized_ollama = None
gpu_ollama = None

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
conversation_history = {}
think_sessions = {}

# Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±
def initialize_qdrant():
    """Qdrant ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±"""
    global qdrant_client
    
    try:
        logger.info("ğŸ”— Qdrant ì„œë²„ ì—°ê²° ì‹œë„...")
        client = QdrantClient(host="localhost", port=6333)
        
        # í—¬ìŠ¤ì²´í¬
        health = client.get_collections()
        logger.info(f"âœ… Qdrant ì„œë²„ ì—°ê²° ì„±ê³µ! ê¸°ì¡´ ì»¬ë ‰ì…˜: {len(health.collections)}ê°œ")
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        collection_names = [col.name for col in health.collections]
        
        if "documents" not in collection_names:
            logger.info("ğŸ“‹ documents ì»¬ë ‰ì…˜ ìƒì„± ì¤‘...")
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            client.create_collection(
                collection_name="documents",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            logger.info("âœ… documents ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
            logger.info("ğŸ“„ ë¹ˆ ì»¬ë ‰ì…˜ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            collection_info = client.get_collection("documents")
            logger.info(f"âœ… documents ì»¬ë ‰ì…˜ ì¡´ì¬: {collection_info.points_count}ê°œ ë¬¸ì„œ")
        
        qdrant_client = client
        return client
        
    except Exception as e:
        logger.error(f"âŒ Qdrant ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        qdrant_client = None
        return None

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
try:
    embedding_model = sentence_transformers.SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    logger.warning(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    embedding_model = None

def embed_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©"""
    if not embedding_model:
        return [0.0] * 384
    
    try:
        embeddings = embedding_model.encode([text])
        return embeddings[0].tolist()
    except Exception as e:
        logger.error(f"ì„ë² ë”© ì˜¤ë¥˜: {e}")
        return [0.0] * 384

# ê²€ìƒ‰ í•¨ìˆ˜
def search_documents(query, limit=5):
    """ì‹¤ì œ Qdrant ê²€ìƒ‰"""
    global qdrant_client
    
    if not qdrant_client:
        logger.warning("âš ï¸ Qdrant ì—°ê²° ì—†ìŒ. ì¬ì—°ê²° ì‹œë„...")
        qdrant_client = initialize_qdrant()
        if not qdrant_client:
            return []
    
    if not embedding_model:
        logger.warning("âš ï¸ ì„ë² ë”© ëª¨ë¸ ì—†ìŒ")
        return []
    
    try:
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        collections = qdrant_client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if "documents" not in collection_names:
            logger.info(f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼: documents ì»¬ë ‰ì…˜ ì—†ìŒ")
            return []
        
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
        query_vector = embed_text(query)
        
        # Qdrant ê²€ìƒ‰ ìˆ˜í–‰
        search_result = qdrant_client.query_points(
            collection_name="documents",
            query=query_vector,
            limit=limit,
            with_payload=True,
            with_vectors=False
        )
        
        if not search_result.points:
            logger.info(f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []
        
        # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for result in search_result.points:
            payload = result.payload
            results.append({
                "filename": payload.get("filename", "unknown.pdf"),
                "content": payload.get("content", "")[:200] + "..." if len(payload.get("content", "")) > 200 else payload.get("content", ""),
                "score": float(result.score),
                "page": payload.get("page", 1),
                "document_type": payload.get("document_type", "ë¬¸ì„œ")
            })
        
        logger.info(f"âœ… '{query}' ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ")
        return results
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# Whisper ëª¨ë¸ ë¡œë“œ
try:
    whisper_model = whisper.load_model("large")
    logger.info("âœ… Whisper large model loaded successfully")
except Exception as e:
    try:
        whisper_model = whisper.load_model("medium")
        logger.info("âœ… Whisper medium model loaded successfully")
    except Exception as e2:
        whisper_model = whisper.load_model("small")
        logger.info("âœ… Whisper small model loaded successfully")

# Ollama ì„¤ì •
OLLAMA_API_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "qwen3:8b"
ALLOWED_AUDIO_EXTENSIONS = {'mp3', 'm4a', 'wav', 'flac', 'aac', 'ogg', 'wma'}

def allowed_audio_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_AUDIO_EXTENSIONS

# GPU ê°€ì† LLM í´ë˜ìŠ¤ (ìˆ˜ì •ëœ ë²„ì „)
class GPUAcceleratedLLM:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = None
        self.initialized = False
        
        try:
            self.initialize_gpu_model()
        except Exception as e:
            logger.warning(f"GPU LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model = None

    def initialize_gpu_model(self):
        """GPU LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            if not torch.cuda.is_available():
                logger.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ê°„ë‹¨í•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
            model_name = "microsoft/DialoGPT-medium"
            
            logger.info(f"GPU LLM ëª¨ë¸ ë¡œë”©: {model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            self.device = next(self.model.parameters()).device
            self.initialized = True
            
            logger.info(f"âœ… GPU LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.device}")
            
        except Exception as e:
            logger.error(f"GPU LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.model = None
            self.tokenizer = None

    def generate_response(self, prompt, max_tokens=512):
        """GPU LLM ì‘ë‹µ ìƒì„±"""
        if not self.initialized or not self.model:
            return None
        
        try:
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # ì›ë˜ í”„ë¡¬í”„íŠ¸ ì œê±°
            response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            logger.error(f"GPU LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return None

# ê¸€ë¡œë²Œ GPU LLM ì¸ìŠ¤í„´ìŠ¤
gpu_llm = None

def query_ollama_streaming(prompt):
    """Ollama ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    try:
        logger.info("ğŸ¤– Ollama ìš”ì²­ ì‹œì‘...")
        
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": 0.1,      # 0.3ì—ì„œ 0.1ë¡œ ê°ì†Œ (ë” ë¹ ë¦„)
                "num_predict": 150,      # 200ì—ì„œ 150ìœ¼ë¡œ ê°ì†Œ
                "num_ctx": 512,          # 1024ì—ì„œ 512ë¡œ ê°ì†Œ
                "num_thread": 8,         # ìŠ¤ë ˆë“œ ìˆ˜ ì¶”ê°€
                "repeat_penalty": 1.1    # ë°˜ë³µ ë°©ì§€
            }
        }
        
        response = requests.post(
            OLLAMA_API_URL, 
            json=payload, 
            timeout=30,
            stream=True
        )
        response.raise_for_status()
        
        full_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    decoded_line = line.decode('utf-8')
                    json_line = json.loads(decoded_line)
                    chunk = json_line.get("response", "")
                    full_response += chunk
                    if json_line.get("done"):
                        break
                except json.JSONDecodeError:
                    continue
        
        # <think> íƒœê·¸ ì œê±°
        if "<think>" in full_response and "</think>" in full_response:
            start_tag = full_response.find("<think>")
            end_tag = full_response.find("</think>") + len("</think>")
            full_response = full_response[:start_tag] + full_response[end_tag:]
        
        full_response = full_response.strip()
        logger.info(f"âœ… Ollama ì‘ë‹µ ì™„ë£Œ: {len(full_response)}ì")
        return full_response
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Ollama ì—°ê²° ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Ollama ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def query_ollama_fast(prompt):
    """ë¹ ë¥¸ Ollama í˜¸ì¶œ"""
    try:
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "num_predict": 200,
                "num_ctx": 1024
            }
        }
        
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=15)
        
        if response.status_code == 200:
            result = response.json()
            return result.get("response", "").strip()
        else:
            return None
            
    except Exception as e:
        logger.error(f"âŒ ë¹ ë¥¸ Ollama ì˜¤ë¥˜: {e}")
        return None

def log_request(request_type, user_id, action, status, processing_time):
    """ìš”ì²­ ë¡œê·¸ ê¸°ë¡"""
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'type': request_type,
        'user': user_id,
        'action': action,
        'status': status,
        'processing_time': processing_time
    }
    
    stats_data['recent_logs'].insert(0, log_entry)
    if len(stats_data['recent_logs']) > 100:
        stats_data['recent_logs'].pop()
    
    stats_data['total_requests'] += 1
    if status == 'success':
        stats_data['successful_requests'] += 1
    else:
        stats_data['failed_requests'] += 1
    
    today = datetime.now().strftime('%Y-%m-%d')
    if today not in stats_data['daily_usage']:
        stats_data['daily_usage'][today] = 0
    stats_data['daily_usage'][today] += 1

# =============================================================================
# ë¼ìš°íŠ¸ ì •ì˜
# =============================================================================

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    try:
        if os.path.exists('index.html'):
            return send_from_directory('.', 'index.html')
        else:
            with open('paste-3.txt', 'r', encoding='utf-8') as f:
                return f.read()
    except Exception as e:
        logger.error(f"ë©”ì¸ í˜ì´ì§€ ì˜¤ë¥˜: {str(e)}")
        return render_template_string("""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <title>EX-GPT ì„œë²„</title>
            <style>
                body { font-family: 'Malgun Gothic', sans-serif; margin: 40px; }
                .status { background: #f0f8ff; padding: 20px; border-radius: 8px; margin: 20px 0; }
                .success { color: #28a745; }
            </style>
        </head>
        <body>
            <h1>ğŸš€ EX-GPT ì„œë²„</h1>
            <div class="status">
                <p class="success">âœ… ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤</p>
            </div>
        </body>
        </html>
        """)

@app.route('/admin')
def admin_dashboard():
    """ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ"""
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ íŒŒì¼ ì°¾ê¸°
        admin_file_path = '/mnt/c/projects/ex-gpt-demo/admin_dashboard.html'
        
        if os.path.exists(admin_file_path):
            with open(admin_file_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            # ìƒëŒ€ ê²½ë¡œë„ ì‹œë„
            if os.path.exists('admin_dashboard.html'):
                with open('admin_dashboard.html', 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                raise FileNotFoundError("ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
    except Exception as e:
        logger.error(f"ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ ì˜¤ë¥˜: {str(e)}")
        return render_template_string("""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <title>EX-GPT ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ</title>
            <style>
                body { font-family: 'Malgun Gothic', sans-serif; margin: 40px; }
                .error { color: #dc3545; background: #f8d7da; padding: 20px; border-radius: 8px; }
            </style>
        </head>
        <body>
            <h1>EX-GPT ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ</h1>
            <div class="error">
                <h3>âš ï¸ ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤</h3>
                <p>admin_dashboard.html íŒŒì¼ì´ ë‹¤ìŒ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤:</p>
                <ul>
                    <li>/mnt/c/projects/ex-gpt-demo/admin_dashboard.html</li>
                    <li>í˜„ì¬ ë””ë ‰í† ë¦¬/admin_dashboard.html</li>
                </ul>
                <p><a href="/">ë©”ì¸ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°</a></p>
            </div>
        </body>
        </html>
        """), 500

@app.route('/api/chat', methods=['POST'])
def enhanced_text_chat():
    """í–¥ìƒëœ í…ìŠ¤íŠ¸ ì±„íŒ… API"""
    start_time = time.time()
    
    try:
        data = request.json
        if not data:
            return jsonify({'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400
            
        message = data.get('message', '').strip()
        user_id = data.get('user_id', request.remote_addr)
        mode = data.get('mode', 'standard')
        
        if not message:
            return jsonify({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        logger.info(f"ğŸ’¬ ì‚¬ìš©ì ë©”ì‹œì§€: '{message}' (ëª¨ë“œ: {mode})")
        
        # í™œì„± ì‚¬ìš©ì ì¶”ê°€
        stats_data['active_users'].add(user_id)
        
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        logger.info(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘: '{message}'")
        relevant_docs = search_documents(message, limit=3)
        logger.info(f"ğŸ“‹ ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
        
        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if mode == 'think':
            # Think ëª¨ë“œ í”„ë¡¬í”„íŠ¸
            if relevant_docs:
                context = "\n".join([
                    f"ğŸ“„ {doc['filename']} (ê´€ë ¨ë„: {doc['score']:.2f})\n{doc['content']}" 
                    for doc in relevant_docs
                ])
                
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

<think>
ë‹¨ê³„ë³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
1. ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•…: ì‚¬ìš©ìê°€ ì •í™•íˆ ë¬´ì—‡ì„ ë¬»ê³  ìˆëŠ”ê°€?
2. ê´€ë ¨ ë¬¸ì„œ í‰ê°€: ì œê³µëœ ë¬¸ì„œë“¤ì´ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ê°€?
3. ì¶”ê°€ ì •ë³´ í•„ìš”ì„±: ë” í•„ìš”í•œ ì •ë³´ê°€ ìˆëŠ”ê°€?
4. ë‹µë³€ êµ¬ì¡°í™”: ì–´ë–¤ ìˆœì„œë¡œ ì„¤ëª…í•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œ?
5. ì‹¤ë¬´ì  ê´€ì : í•œêµ­ë„ë¡œê³µì‚¬ ì§ì›ì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì¸ê°€?
</think>

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ì‚¬ìš©ì ì§ˆë¬¸ ===
{message}

=== ë‹µë³€ ì§€ì¹¨ ===
1. <think> íƒœê·¸ ì•ˆì—ì„œ ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •ì„ ìƒì„¸íˆ ê¸°ë¡í•˜ì„¸ìš”
2. ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…í™•íˆ í‘œì‹œí•˜ê³  ì¶”ê°€ í™•ì¸ì„ ê¶Œí•˜ì„¸ìš”
4. í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ë°˜ì˜í•œ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€:"""
            else:
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

<think>
ë‹¨ê³„ë³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
1. ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•…: {message}
2. ë„ë¡œê³µì‚¬ ê´€ë ¨ ì§€ì‹ í™œìš©
3. ì‹¤ë¬´ì  ë‹µë³€ êµ¬ì¡°í™”
4. ì¶”ê°€ ë„ì›€ ë°©ì•ˆ ì œì‹œ
</think>

ì‚¬ìš©ì ì§ˆë¬¸: {message}

ë„ë¡œ, êµí†µ, ê³ ì†ë„ë¡œì™€ ê´€ë ¨ëœ ì „ë¬¸ ì§€ì‹ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
        else:
            # ì¼ë°˜ ëª¨ë“œ í”„ë¡¬í”„íŠ¸
            if relevant_docs:
                context = "\n".join([
                    f"ğŸ“„ {doc['filename']} (ê´€ë ¨ë„: {doc['score']:.2f})\n{doc['content']}" 
                    for doc in relevant_docs
                ])
                
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì„œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ì‚¬ìš©ì ì§ˆë¬¸ ===
{message}

=== ë‹µë³€ ì§€ì¹¨ ===
1. ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë˜, ì¶”ì¸¡ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ì„±ì„ ì‚´ë ¤ ë‹µë³€í•˜ì„¸ìš”
4. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”

ë‹µë³€:"""
            else:
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {message}

í˜„ì¬ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì§€ë§Œ, ë„ë¡œ, êµí†µ, ê³ ì†ë„ë¡œì™€ ê´€ë ¨ëœ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ì„±ì„ ì‚´ë ¤ ì¹œê·¼í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
        
        # 3. ë¹ ë¥¸ ëª¨ë“œ ë˜ëŠ” ì¼ë°˜ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„±
        # standard ëª¨ë“œì¼ ë•Œ ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        if mode == 'think':
            ai_response = query_ollama_streaming(prompt)
        else:
            # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë¹ ë¥¸ ì‘ë‹µ
            simple_prompt = f"í•œêµ­ë„ë¡œê³µì‚¬ AIì…ë‹ˆë‹¤. ê°„ë‹¨íˆ ë‹µë³€í•˜ì„¸ìš”: {message}"
            ai_response = query_ollama_fast(simple_prompt)
        
        if not ai_response:
            if relevant_docs:
                ai_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ëª¨ë¸ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\n\ní•˜ì§€ë§Œ '{message}'ì™€ ê´€ë ¨ëœ ë¬¸ì„œ {len(relevant_docs)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ ì¶œì²˜ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”."
            else:
                ai_response = f"ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ë„ë¡œê³µì‚¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n'{message}'ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦¬ê³  ì‹¶ì§€ë§Œ, í˜„ì¬ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ê³  AI ëª¨ë¸ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # Think ëª¨ë“œ ì‘ë‹µ ì²˜ë¦¬
        thinking_process = None
        if mode == 'think' and ai_response:
            if "<think>" in ai_response and "</think>" in ai_response:
                start_idx = ai_response.find("<think>") + len("<think>")
                end_idx = ai_response.find("</think>")
                thinking_process = ai_response[start_idx:end_idx].strip()
                ai_response = ai_response[ai_response.find("</think>") + len("</think>"):].strip()
        
        processing_time = time.time() - start_time
        
        # 4. ì¶œì²˜ ì •ë³´ êµ¬ì„±
        sources = []
        if relevant_docs:
            sources = [
                {
                    "title": doc["filename"],
                    "content_preview": doc["content"],
                    "relevance_score": round(doc["score"], 3),
                    "page": doc.get("page", 1),
                    "document_type": doc.get("document_type", "ë¬¸ì„œ")
                }
                for doc in relevant_docs
            ]
        
        # 5. ë¡œê·¸ ê¸°ë¡
        log_request('í…ìŠ¤íŠ¸', user_id, f'ëª¨ë“œ: {mode}', 'success', f"{processing_time:.2f}ì´ˆ")
        
        response_data = {
            'reply': ai_response,
            'sources': sources,
            'status': 'success',
            'processing_time': f"{processing_time:.2f}ì´ˆ",
            'documents_found': len(relevant_docs),
            'mode': mode,
            'timestamp': datetime.now().isoformat()
        }
        
        # Think ëª¨ë“œì¸ ê²½ìš° ì‚¬ê³  ê³¼ì • ì¶”ê°€
        if thinking_process:
            response_data['thinking_process'] = thinking_process
        
        return jsonify(response_data)
        
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        
        log_request('í…ìŠ¤íŠ¸', request.remote_addr, 'ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
        
        return jsonify({
            'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
            'status': 'error',
            'processing_time': f"{processing_time:.2f}ì´ˆ"
        }), 500

@app.route('/api/chat_stream', methods=['POST'])
def chat_stream():
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… API"""
    def generate():
        try:
            data = request.json
            message = data.get('message', '').strip()
            mode = data.get('mode', 'standard')
            
            # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
            if mode == 'think':
                prompt = f"<think>ìƒê°: {message}ì— ëŒ€í•´ ë¶„ì„</think>\ní•œêµ­ë„ë¡œê³µì‚¬ AIì…ë‹ˆë‹¤. {message}"
            else:
                prompt = f"í•œêµ­ë„ë¡œê³µì‚¬ AIì…ë‹ˆë‹¤. ê°„ë‹¨íˆ: {message}"
            
            # Ollama ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            payload = {
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 100,
                    "num_ctx": 256
                }
            }
            
            response = requests.post(OLLAMA_API_URL, json=payload, stream=True, timeout=30)
            
            for line in response.iter_lines():
                if line:
                    try:
                        json_line = json.loads(line.decode('utf-8'))
                        chunk = json_line.get("response", "")
                        if chunk:
                            yield f"data: {chunk}\n\n"
                        if json_line.get("done"):
                            break
                    except:
                        continue
                        
        except Exception as e:
            yield f"data: ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n"
    
    return Response(generate(), mimetype='text/plain')

@app.route('/api/admin/dashboard_data', methods=['GET'])
def get_dashboard_data():
    """ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ë°ì´í„°"""
    try:
        # GPU ìƒíƒœ
        gpu_info = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                device = f"cuda:{i}"
                memory_allocated = torch.cuda.memory_allocated(device)
                memory_total = torch.cuda.get_device_properties(device).total_memory
                
                gpu_info.append({
                    'device': device,
                    'memory_used_gb': memory_allocated / 1024**3,
                    'memory_total_gb': memory_total / 1024**3,
                    'utilization_percent': (memory_allocated / memory_total) * 100
                })
        
        # ì‹œìŠ¤í…œ í†µê³„
        total_requests = stats_data['total_requests']
        active_users = len(stats_data['active_users'])
        
        # ë¬¸ì„œ í†µê³„
        document_count = 0
        if qdrant_client:
            try:
                collection_info = qdrant_client.get_collection("documents")
                document_count = collection_info.points_count
            except:
                pass
        
        return jsonify({
            'total_requests': total_requests,
            'active_users': active_users,
            'gpu_usage': gpu_info,
            'document_count': document_count,
            'target_document_count': 2199,
            'system_uptime': "99.8%",
            'upload_sessions': len(upload_sessions),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/stats', methods=['GET'])
def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ë°ì´í„° ë°˜í™˜"""
    try:
        current_active = len(stats_data['active_users'])
        
        today = datetime.now()
        daily_data = {}
        for i in range(15):
            date = (today - timedelta(days=i)).strftime('%Y-%m-%d')
            daily_data[date] = stats_data['daily_usage'].get(date, 0)
        
        total = stats_data['total_requests']
        success_rate = (stats_data['successful_requests'] / total * 100) if total > 0 else 100
        
        return jsonify({
            'total_requests': stats_data['total_requests'],
            'successful_requests': stats_data['successful_requests'], 
            'failed_requests': stats_data['failed_requests'],
            'success_rate': f"{success_rate:.1f}%",
            'active_users': current_active,
            'daily_usage': daily_data,
            'recent_logs': stats_data['recent_logs'][:20],
            'gpu_usage_history': stats_data['gpu_usage_history'][-50:],
            'system_uptime': "99.7%",
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"í†µê³„ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        return jsonify({
            'status': 'healthy',
            'whisper_loaded': whisper_model is not None,
            'qdrant_connected': qdrant_client is not None,
            'embedding_model_loaded': embedding_model is not None,
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'total_requests': stats_data['total_requests'],
            'active_users': len(stats_data['active_users']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

@app.route('/api/qdrant_status', methods=['GET'])
def qdrant_status():
    """Qdrant ìƒíƒœ í™•ì¸ API"""
    try:
        if not qdrant_client:
            return jsonify({"status": "disconnected", "error": "í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"})
        
        collections = qdrant_client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if "documents" in collection_names:
            collection_info = qdrant_client.get_collection("documents")
            return jsonify({
                "status": "connected",
                "collection_exists": True,
                "points_count": collection_info.points_count,
                "vectors_count": collection_info.vectors_count
            })
        else:
            return jsonify({
                "status": "connected",
                "collection_exists": False,
                "error": "documents ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            })
            
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

@app.route('/api/test', methods=['GET']) 
def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        "message": "EX-GPT ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.", 
        "whisper_loaded": whisper_model is not None,
        "qdrant_connected": qdrant_client is not None,
        "embedding_loaded": embedding_model is not None,
        "endpoints": [
            "/api/chat",
            "/api/stats",
            "/api/health",
            "/api/test"
        ]
    })

# ì˜¤ë¥˜ í•¸ë“¤ëŸ¬
@app.route('/favicon.ico')
def favicon():
    return '', 204

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Endpoint not found'}), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(error)}")
    return jsonify({'error': 'Internal server error'}), 500

# ì£¼ê¸°ì  ì •ë¦¬ ì‘ì—…
def cleanup_old_data():
    """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
    if len(stats_data['recent_logs']) > 1000:
        stats_data['recent_logs'] = stats_data['recent_logs'][:500]
    
    if len(stats_data['gpu_usage_history']) > 2000:
        stats_data['gpu_usage_history'] = stats_data['gpu_usage_history'][-1000:]
    
    # í™œì„± ì‚¬ìš©ì ëª©ë¡ ì •ë¦¬
    stats_data['active_users'].clear()

def periodic_cleanup():
    """ì£¼ê¸°ì  ì •ë¦¬ ì‹¤í–‰"""
    while True:
        time.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤
        cleanup_old_data()

# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

if __name__ == '__main__':
    logger.info("ğŸš€ EX-GPT ì„œë²„ ì‹œì‘...")
    
    # 1. CUDA í™˜ê²½ ê²€ì¦
    if not torch.cuda.is_available():
        logger.warning("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # 2. Qdrant ì´ˆê¸°í™”
    logger.info("ğŸ”— Qdrant ì´ˆê¸°í™”...")
    qdrant_client = initialize_qdrant()
    if qdrant_client:
        logger.info("âœ… Qdrant ì¤€ë¹„ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ Qdrant ì´ˆê¸°í™” ì‹¤íŒ¨. ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

    # 3. GPU LLM ì´ˆê¸°í™”
    try:
        logger.info("ğŸš€ GPU LLM ì´ˆê¸°í™”...")
        gpu_llm = GPUAcceleratedLLM()
        if gpu_llm.initialized:
            logger.info("âœ… GPU LLM ì¤€ë¹„ ì™„ë£Œ")
        else:
            logger.info("âœ… GPU LLM ì´ˆê¸°í™” ì™„ë£Œ (Ollama í´ë°±)")
    except Exception as e:
        logger.warning(f"âš ï¸ GPU LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        gpu_llm = None
    
    # 4. ì •ë¦¬ ì‘ì—… ì‹œì‘
    cleanup_thread = threading.Thread(target=periodic_cleanup, daemon=True)
    cleanup_thread.start()
    
    logger.info("ğŸ‰ ëª¨ë“  ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    logger.info("ğŸ“ ì„œë²„ ì£¼ì†Œ: http://localhost:5001")
    logger.info("ğŸ“Š ê´€ë¦¬ì: http://localhost:5001/admin")
    logger.info("ğŸ” ê¸°ëŠ¥:")
    logger.info("   - âœ… ì‹¤ì‹œê°„ Ollama LLM ì—°ë™")
    logger.info("   - âœ… Qdrant ë¬¸ì„œ ê²€ìƒ‰")
    logger.info("   - âœ… ìŒì„± ì¸ì‹ ë° ìš”ì•½")
    
    app.run(host='0.0.0.0', port=5001, debug=False, threaded=True)