#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from flask import Flask, request, jsonify, send_from_directory, render_template_string
from flask_cors import CORS
import torch
import torch.nn as nn
import os
import gc
import logging
import threading
import time
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import asyncio
from functools import wraps
import warnings
import whisper
import tempfile
from werkzeug.utils import secure_filename
import requests
import json
import librosa
import noisereduce as nr
from pydub import AudioSegment
from pydub.silence import split_on_silence
import numpy as np
from datetime import datetime, timedelta
import uuid
import sentence_transformers
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import uuid
import asyncio
from concurrent.futures import ThreadPoolExecutor
import queue
import threading
from tqdm import tqdm
import json
import zipfile
import os
import tempfile
from pathlib import Path
import subprocess
import psutil
import threading
from queue import Queue, Empty
import asyncio
from concurrent.futures import ThreadPoolExecutor
from flask import Response
# CUDA PyTorch í™˜ê²½ì—ì„œ ë™ì‘í•˜ëŠ” ìŒì„± ì²˜ë¦¬ ì‹œìŠ¤í…œ

import torch
import torch.nn as nn
import os
import gc
import logging
import threading
import time
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import tempfile
import requests
import json
import librosa
import noisereduce as nr
from pydub import AudioSegment
from pydub.silence import split_on_silence
import numpy as np
from datetime import datetime, timedelta
import uuid
# ìŒì„± ì²˜ë¦¬ ê´€ë ¨ imports
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
    logger.info("âœ… Faster Whisper ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    logger.warning("âš ï¸ Faster Whisper ì‚¬ìš© ë¶ˆê°€")

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
    logger.info("âœ… SpeechRecognition ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    logger.warning("âš ï¸ SpeechRecognition ì‚¬ìš© ë¶ˆê°€")

try:
    from transformers import WhisperProcessor, WhisperForConditionalGeneration
    TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… Transformers Whisper ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("âš ï¸ Transformers ì‚¬ìš© ë¶ˆê°€")

# ============= Flask ì•± ë° CORS ì„¤ì • =============
app = Flask(__name__)
CORS(app)

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ë³€ìˆ˜ - ë“€ì–¼ GPU ì§€ì›
florence_models = {}
florence_processors = {}
available_devices = ["cuda:0", "cuda:1"]
request_queue = Queue()
gpu_load_tracker = {device: 0 for device in available_devices}
gpu_lock = threading.Lock()

# í†µê³„ ë°ì´í„° ì €ì¥
stats_data = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'active_users': set(),
    'daily_usage': {},
    'gpu_usage_history': [],
    'recent_logs': []
}

# ì—…ë¡œë“œ ì„¸ì…˜ ê´€ë¦¬
upload_sessions = {}

# ============================================================================
# STEP 2: ì „ì—­ ë³€ìˆ˜ ì„¹ì…˜ì— ì¶”ê°€
# ============================================================================
# ê¸€ë¡œë²Œ ë³€ìˆ˜ ì„ ì–¸
qdrant_client = None
embedding_model = None
whisper_model = None
optimized_ollama = None
gpu_ollama = None

# ìŒì„± ì²˜ë¦¬ ëª¨ë¸ë“¤
faster_whisper_model = None
transformers_whisper_model = None
transformers_processor = None
speech_recognizer = None


# ìŒì„± ì²˜ë¦¬ ì„¤ì •
AUDIO_SETTINGS = {
    'sample_rate': 16000,
    'channels': 1,
    'bit_depth': 16,
    'format': 'wav'
}

WHISPER_OPTIONS = {
    'language': 'ko',
    'temperature': 0.0,
    'beam_size': 5,
    'condition_on_previous_text': True,
    'compression_ratio_threshold': 2.4,
    'logprob_threshold': -1.0,
    'no_speech_threshold': 0.6
}

# ìŒì„± ì²˜ë¦¬ ì„¸ì…˜ ê´€ë¦¬
audio_processing_sessions = {}
audio_processing_lock = Lock()

# ============================================================================
# STEP 3: ìŒì„± ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ì¶”ê°€
# ============================================================================


# ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
conversation_history = {}
think_sessions = {}

# Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±
def initialize_qdrant():
    """Qdrant ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±"""
    global qdrant_client
    
    try:
        logger.info("ğŸ”— Qdrant ì„œë²„ ì—°ê²° ì‹œë„...")
        client = QdrantClient(host="localhost", port=6333)
        
        # í—¬ìŠ¤ì²´í¬
        health = client.get_collections()
        logger.info(f"âœ… Qdrant ì„œë²„ ì—°ê²° ì„±ê³µ! ê¸°ì¡´ ì»¬ë ‰ì…˜: {len(health.collections)}ê°œ")
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        collection_names = [col.name for col in health.collections]
        
        if "documents" not in collection_names:
            logger.info("ğŸ“‹ documents ì»¬ë ‰ì…˜ ìƒì„± ì¤‘...")
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            client.create_collection(
                collection_name="documents",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            logger.info("âœ… documents ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
            logger.info("ğŸ“„ ë¹ˆ ì»¬ë ‰ì…˜ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            collection_info = client.get_collection("documents")
            logger.info(f"âœ… documents ì»¬ë ‰ì…˜ ì¡´ì¬: {collection_info.points_count}ê°œ ë¬¸ì„œ")
        
        qdrant_client = client
        return client
        
    except Exception as e:
        logger.error(f"âŒ Qdrant ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        qdrant_client = None
        return None

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
try:
    embedding_model = sentence_transformers.SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    logger.warning(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    embedding_model = None

def embed_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©"""
    if not embedding_model:
        return [0.0] * 384
    
    try:
        embeddings = embedding_model.encode([text])
        return embeddings[0].tolist()
    except Exception as e:
        logger.error(f"ì„ë² ë”© ì˜¤ë¥˜: {e}")
        return [0.0] * 384

# ê²€ìƒ‰ í•¨ìˆ˜
def search_documents(query, limit=5):
    """ì‹¤ì œ Qdrant ê²€ìƒ‰"""
    global qdrant_client
    
    if not qdrant_client:
        logger.warning("âš ï¸ Qdrant ì—°ê²° ì—†ìŒ. ì¬ì—°ê²° ì‹œë„...")
        qdrant_client = initialize_qdrant()
        if not qdrant_client:
            return []
    
    if not embedding_model:
        logger.warning("âš ï¸ ì„ë² ë”© ëª¨ë¸ ì—†ìŒ")
        return []
    
    try:
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        collections = qdrant_client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if "documents" not in collection_names:
            logger.info(f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼: documents ì»¬ë ‰ì…˜ ì—†ìŒ")
            return []
        
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
        query_vector = embed_text(query)
        
        # Qdrant ê²€ìƒ‰ ìˆ˜í–‰
        search_result = qdrant_client.query_points(
            collection_name="documents",
            query=query_vector,
            limit=limit,
            with_payload=True,
            with_vectors=False
        )
        
        if not search_result.points:
            logger.info(f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []
        
        # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for result in search_result.points:
            payload = result.payload
            results.append({
                "filename": payload.get("filename", "unknown.pdf"),
                "content": payload.get("content", "")[:200] + "..." if len(payload.get("content", "")) > 200 else payload.get("content", ""),
                "score": float(result.score),
                "page": payload.get("page", 1),
                "document_type": payload.get("document_type", "ë¬¸ì„œ")
            })
        
        logger.info(f"âœ… '{query}' ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ")
        return results
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# 1. Faster Whisper ì‚¬ìš© (ê¸°ì¡´ PyTorchì™€ ì¶©ëŒ ì—†ìŒ)
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
    logger.info("âœ… Faster Whisper ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    logger.warning("âš ï¸ Faster Whisper ì‚¬ìš© ë¶ˆê°€")

# 2. SpeechRecognition ë°±ì—… ì˜µì…˜
try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
    logger.info("âœ… SpeechRecognition ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    logger.warning("âš ï¸ SpeechRecognition ì‚¬ìš© ë¶ˆê°€")

# 3. Hugging Face Transformers ì‚¬ìš© (PyTorch ê¸°ë°˜)
try:
    from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration
    TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… Transformers Whisper ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("âš ï¸ Transformers ì‚¬ìš© ë¶ˆê°€")

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜ë“¤
faster_whisper_model = None
transformers_whisper_model = None
transformers_processor = None
speech_recognizer = None

# Ollama ì„¤ì •
OLLAMA_API_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "qwen3:8b"
ALLOWED_AUDIO_EXTENSIONS = {'mp3', 'm4a', 'wav', 'flac', 'aac', 'ogg', 'wma'}



def transcribe_with_faster_whisper(audio_path):
    """Faster Whisperë¡œ ìŒì„± ì¸ì‹"""
    if not faster_whisper_model:
        return None
    
    try:
        logger.info("ğŸ”„ Faster Whisper STT ì‹œì‘...")
        
        segments, info = faster_whisper_model.transcribe(
            audio_path,
            language="ko",
            beam_size=5,
            temperature=0.0,
            condition_on_previous_text=True,
            compression_ratio_threshold=2.4,
            logprob_threshold=-1.0,
            no_speech_threshold=0.6
        )
        
        # ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        transcription = ""
        for segment in segments:
            transcription += segment.text + " "
        
        transcription = transcription.strip()
        logger.info(f"âœ… Faster Whisper STT ì™„ë£Œ: {len(transcription)}ì")
        return transcription
        
    except Exception as e:
        logger.error(f"âŒ Faster Whisper STT ì˜¤ë¥˜: {e}")
        return None

def transcribe_with_transformers(audio_path):
    """Transformers Whisperë¡œ ìŒì„± ì¸ì‹"""
    if not transformers_whisper_model or not transformers_processor:
        return None
    
    try:
        logger.info("ğŸ”„ Transformers Whisper STT ì‹œì‘...")
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
        audio, sr_rate = librosa.load(audio_path, sr=16000)
        
        # ì…ë ¥ íŠ¹ì„± ì¶”ì¶œ
        input_features = transformers_processor(
            audio, 
            sampling_rate=16000, 
            return_tensors="pt"
        ).input_features
        
        # GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            input_features = input_features.to("cuda")
        
        # ê°•ì œë¡œ í•œêµ­ì–´ í† í° ì„¤ì •
        forced_decoder_ids = transformers_processor.get_decoder_prompt_ids(
            language="korean", 
            task="transcribe"
        )
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            predicted_ids = transformers_whisper_model.generate(
                input_features, 
                forced_decoder_ids=forced_decoder_ids,
                max_length=448,
                temperature=0.0,
                do_sample=False
            )
        
        # ë””ì½”ë”©
        transcription = transformers_processor.batch_decode(
            predicted_ids, 
            skip_special_tokens=True
        )[0]
        
        logger.info(f"âœ… Transformers Whisper STT ì™„ë£Œ: {len(transcription)}ì")
        return transcription.strip()
        
    except Exception as e:
        logger.error(f"âŒ Transformers Whisper STT ì˜¤ë¥˜: {e}")
        return None

def allowed_audio_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_AUDIO_EXTENSIONS

# ============================================================================
# STEP 3: ìŒì„± ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ì¶”ê°€
# ============================================================================

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ ì•„ë˜ì— ì¶”ê°€í•˜ì„¸ìš”:

def initialize_audio_models():
    """ìŒì„± ì²˜ë¦¬ ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
    global faster_whisper_model, transformers_whisper_model, transformers_processor, speech_recognizer
    
    models_loaded = []
    
    # Faster Whisper ì´ˆê¸°í™”
    if FASTER_WHISPER_AVAILABLE:
        try:
            if torch.cuda.is_available():
                faster_whisper_model = WhisperModel("medium", device="cuda", compute_type="float16")
                logger.info("âœ… Faster Whisper GPU ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            else:
                faster_whisper_model = WhisperModel("medium", device="cpu", compute_type="int8")
                logger.info("âœ… Faster Whisper CPU ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            models_loaded.append("faster_whisper")
        except Exception as e:
            logger.error(f"âŒ Faster Whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # SpeechRecognition ì´ˆê¸°í™”
    if SPEECH_RECOGNITION_AVAILABLE:
        try:
            speech_recognizer = sr.Recognizer()
            models_loaded.append("speech_recognition")
            logger.info("âœ… SpeechRecognition ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ SpeechRecognition ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    logger.info(f"ğŸ¤ ì‚¬ìš© ê°€ëŠ¥í•œ STT ëª¨ë¸: {', '.join(models_loaded)}")
    return len(models_loaded) > 0

def transcribe_with_faster_whisper(audio_path):
    """Faster Whisperë¡œ ìŒì„± ì¸ì‹"""
    if not faster_whisper_model:
        return None
    
    try:
        segments, info = faster_whisper_model.transcribe(
            audio_path,
            language="ko",
            beam_size=5,
            temperature=0.0
        )
        
        transcription = ""
        for segment in segments:
            transcription += segment.text + " "
        
        return transcription.strip()
        
    except Exception as e:
        logger.error(f"Faster Whisper STT ì˜¤ë¥˜: {e}")
        return None

def transcribe_with_speech_recognition(audio_path):
    """SpeechRecognitionìœ¼ë¡œ ìŒì„± ì¸ì‹"""
    if not speech_recognizer:
        return None
    
    try:
        # WAV íŒŒì¼ë¡œ ë³€í™˜
        audio = AudioSegment.from_file(audio_path)
        wav_path = audio_path.rsplit('.', 1)[0] + '_sr.wav'
        audio.export(wav_path, format="wav")
        
        # ìŒì„± ì¸ì‹
        with sr.AudioFile(wav_path) as source:
            audio_data = speech_recognizer.record(source)
        
        transcription = speech_recognizer.recognize_google(
            audio_data, 
            language='ko-KR'
        )
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(wav_path):
            os.remove(wav_path)
        
        return transcription
        
    except Exception as e:
        logger.error(f"SpeechRecognition STT ì˜¤ë¥˜: {e}")
        return None

def transcribe_audio(audio_path):
    """í†µí•© ìŒì„± ì¸ì‹ í•¨ìˆ˜"""
    # Faster Whisper ìš°ì„  ì‹œë„
    if faster_whisper_model:
        result = transcribe_with_faster_whisper(audio_path)
        if result:
            return enhance_transcription_quality(result)
    
    # ë°±ì—…ìœ¼ë¡œ SpeechRecognition ì‹œë„
    if speech_recognizer:
        result = transcribe_with_speech_recognition(audio_path)
        if result:
            return enhance_transcription_quality(result)
    
    return None

def enhance_transcription_quality(text):
    """ì „ì‚¬ í’ˆì§ˆ í–¥ìƒ"""
    if not text:
        return text
    
    import re
    
    # ê¸°ë³¸ ì •ë¦¬
    text = text.strip()
    
    # ë°˜ë³µ êµ¬ë¬¸ ì œê±°
    sentences = text.split('. ')
    unique_sentences = []
    prev_sentence = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence != prev_sentence:
            unique_sentences.append(sentence)
            prev_sentence = sentence
    
    text = '. '.join(unique_sentences)
    
    # ë¬¸ì¥ ë¶€í˜¸ ì •ë¦¬
    text = re.sub(r'\s+([,.!?])', r'\1', text)
    text = re.sub(r'([,.!?])\s*([,.!?])', r'\1', text)
    text = re.sub(r'\s+', ' ', text)
    
    if text and not text.endswith(('.', '!', '?')):
        text += '.'
    
    return text

def create_processing_session(user_id):
    """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì„¸ì…˜ ìƒì„±"""
    session_id = str(uuid.uuid4())
    audio_processing_sessions[session_id] = {
        'user_id': user_id,
        'status': 'initialized',
        'progress': 0,
        'message': 'ì²˜ë¦¬ ì¤€ë¹„ ì¤‘...',
        'created_at': datetime.now(),
        'result': None
    }
    return session_id

def update_processing_session(session_id, status=None, progress=None, message=None, result=None):
    """ì²˜ë¦¬ ì„¸ì…˜ ì—…ë°ì´íŠ¸"""
    if session_id in audio_processing_sessions:
        session = audio_processing_sessions[session_id]
        if status: session['status'] = status
        if progress is not None: session['progress'] = progress
        if message: session['message'] = message
        if result: session['result'] = result
        session['updated_at'] = datetime.now()
        
# GPU ê°€ì† LLM í´ë˜ìŠ¤ (ìˆ˜ì •ëœ ë²„ì „)
class GPUAcceleratedLLM:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = None
        self.initialized = False
        
        try:
            self.initialize_gpu_model()
        except Exception as e:
            logger.warning(f"GPU LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model = None

    def initialize_gpu_model(self):
        """GPU LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            if not torch.cuda.is_available():
                logger.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ê°„ë‹¨í•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
            model_name = "microsoft/DialoGPT-medium"
            
            logger.info(f"GPU LLM ëª¨ë¸ ë¡œë”©: {model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            self.device = next(self.model.parameters()).device
            self.initialized = True
            
            logger.info(f"âœ… GPU LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.device}")
            
        except Exception as e:
            logger.error(f"GPU LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.model = None
            self.tokenizer = None

    def generate_response(self, prompt, max_tokens=512):
        """GPU LLM ì‘ë‹µ ìƒì„±"""
        if not self.initialized or not self.model:
            return None
        
        try:
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # ì›ë˜ í”„ë¡¬í”„íŠ¸ ì œê±°
            response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            logger.error(f"GPU LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return None

# ê¸€ë¡œë²Œ GPU LLM ì¸ìŠ¤í„´ìŠ¤
gpu_llm = None

def query_ollama_streaming(prompt):
    """Ollama ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    try:
        logger.info("ğŸ¤– Ollama ìš”ì²­ ì‹œì‘...")
        
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": 0.1,      # 0.3ì—ì„œ 0.1ë¡œ ê°ì†Œ (ë” ë¹ ë¦„)
                "num_predict": 150,      # 200ì—ì„œ 150ìœ¼ë¡œ ê°ì†Œ
                "num_ctx": 512,          # 1024ì—ì„œ 512ë¡œ ê°ì†Œ
                "num_thread": 8,         # ìŠ¤ë ˆë“œ ìˆ˜ ì¶”ê°€
                "repeat_penalty": 1.1    # ë°˜ë³µ ë°©ì§€
            }
        }
        
        response = requests.post(
            OLLAMA_API_URL, 
            json=payload, 
            timeout=30,
            stream=True
        )
        response.raise_for_status()
        
        full_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    decoded_line = line.decode('utf-8')
                    json_line = json.loads(decoded_line)
                    chunk = json_line.get("response", "")
                    full_response += chunk
                    if json_line.get("done"):
                        break
                except json.JSONDecodeError:
                    continue
        
        # <think> íƒœê·¸ ì œê±°
        if "<think>" in full_response and "</think>" in full_response:
            start_tag = full_response.find("<think>")
            end_tag = full_response.find("</think>") + len("</think>")
            full_response = full_response[:start_tag] + full_response[end_tag:]
        
        full_response = full_response.strip()
        logger.info(f"âœ… Ollama ì‘ë‹µ ì™„ë£Œ: {len(full_response)}ì")
        return full_response
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Ollama ì—°ê²° ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Ollama ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def query_ollama_fast(prompt):
    """ë¹ ë¥¸ Ollama í˜¸ì¶œ"""
    try:
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "num_predict": 200,
                "num_ctx": 1024
            }
        }
        
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=15)
        
        if response.status_code == 200:
            result = response.json()
            return result.get("response", "").strip()
        else:
            return None
            
    except Exception as e:
        logger.error(f"âŒ ë¹ ë¥¸ Ollama ì˜¤ë¥˜: {e}")
        return None

def log_request(request_type, user_id, action, status, processing_time):
    """ìš”ì²­ ë¡œê·¸ ê¸°ë¡"""
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'type': request_type,
        'user': user_id,
        'action': action,
        'status': status,
        'processing_time': processing_time
    }
    
    stats_data['recent_logs'].insert(0, log_entry)
    if len(stats_data['recent_logs']) > 100:
        stats_data['recent_logs'].pop()
    
    stats_data['total_requests'] += 1
    if status == 'success':
        stats_data['successful_requests'] += 1
    else:
        stats_data['failed_requests'] += 1
    
    today = datetime.now().strftime('%Y-%m-%d')
    if today not in stats_data['daily_usage']:
        stats_data['daily_usage'][today] = 0
    stats_data['daily_usage'][today] += 1

# =============================================================================
# ë¼ìš°íŠ¸ ì •ì˜
# =============================================================================

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    try:
        if os.path.exists('index.html'):
            return send_from_directory('.', 'index.html')
        else:
            with open('paste-3.txt', 'r', encoding='utf-8') as f:
                return f.read()
    except Exception as e:
        logger.error(f"ë©”ì¸ í˜ì´ì§€ ì˜¤ë¥˜: {str(e)}")
        return render_template_string("""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <title>EX-GPT ì„œë²„</title>
            <style>
                body { font-family: 'Malgun Gothic', sans-serif; margin: 40px; }
                .status { background: #f0f8ff; padding: 20px; border-radius: 8px; margin: 20px 0; }
                .success { color: #28a745; }
            </style>
        </head>
        <body>
            <h1>ğŸš€ EX-GPT ì„œë²„</h1>
            <div class="status">
                <p class="success">âœ… ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤</p>
            </div>
        </body>
        </html>
        """)

@app.route('/admin')
def admin_dashboard():
    """ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ"""
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ íŒŒì¼ ì°¾ê¸°
        admin_file_path = '/mnt/c/projects/ex-gpt-demo/admin_dashboard.html'
        
        if os.path.exists(admin_file_path):
            with open(admin_file_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            # ìƒëŒ€ ê²½ë¡œë„ ì‹œë„
            if os.path.exists('admin_dashboard.html'):
                with open('admin_dashboard.html', 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                raise FileNotFoundError("ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
    except Exception as e:
        logger.error(f"ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ ì˜¤ë¥˜: {str(e)}")
        return render_template_string("""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <title>EX-GPT ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ</title>
            <style>
                body { font-family: 'Malgun Gothic', sans-serif; margin: 40px; }
                .error { color: #dc3545; background: #f8d7da; padding: 20px; border-radius: 8px; }
            </style>
        </head>
        <body>
            <h1>EX-GPT ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ</h1>
            <div class="error">
                <h3>âš ï¸ ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤</h3>
                <p>admin_dashboard.html íŒŒì¼ì´ ë‹¤ìŒ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤:</p>
                <ul>
                    <li>/mnt/c/projects/ex-gpt-demo/admin_dashboard.html</li>
                    <li>í˜„ì¬ ë””ë ‰í† ë¦¬/admin_dashboard.html</li>
                </ul>
                <p><a href="/">ë©”ì¸ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°</a></p>
            </div>
        </body>
        </html>
        """), 500

@app.route('/api/chat', methods=['POST'])
def enhanced_text_chat():
    """í–¥ìƒëœ í…ìŠ¤íŠ¸ ì±„íŒ… API"""
    start_time = time.time()
    
    try:
        data = request.json
        if not data:
            return jsonify({'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400
            
        message = data.get('message', '').strip()
        user_id = data.get('user_id', request.remote_addr)
        mode = data.get('mode', 'standard')
        
        if not message:
            return jsonify({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        logger.info(f"ğŸ’¬ ì‚¬ìš©ì ë©”ì‹œì§€: '{message}' (ëª¨ë“œ: {mode})")
        
        # í™œì„± ì‚¬ìš©ì ì¶”ê°€
        stats_data['active_users'].add(user_id)
        
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        logger.info(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘: '{message}'")
        relevant_docs = search_documents(message, limit=3)
        logger.info(f"ğŸ“‹ ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
        
        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if mode == 'think':
            # Think ëª¨ë“œ í”„ë¡¬í”„íŠ¸
            if relevant_docs:
                context = "\n".join([
                    f"ğŸ“„ {doc['filename']} (ê´€ë ¨ë„: {doc['score']:.2f})\n{doc['content']}" 
                    for doc in relevant_docs
                ])
                
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

<think>
ë‹¨ê³„ë³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
1. ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•…: ì‚¬ìš©ìê°€ ì •í™•íˆ ë¬´ì—‡ì„ ë¬»ê³  ìˆëŠ”ê°€?
2. ê´€ë ¨ ë¬¸ì„œ í‰ê°€: ì œê³µëœ ë¬¸ì„œë“¤ì´ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ê°€?
3. ì¶”ê°€ ì •ë³´ í•„ìš”ì„±: ë” í•„ìš”í•œ ì •ë³´ê°€ ìˆëŠ”ê°€?
4. ë‹µë³€ êµ¬ì¡°í™”: ì–´ë–¤ ìˆœì„œë¡œ ì„¤ëª…í•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œ?
5. ì‹¤ë¬´ì  ê´€ì : í•œêµ­ë„ë¡œê³µì‚¬ ì§ì›ì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì¸ê°€?
</think>

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ì‚¬ìš©ì ì§ˆë¬¸ ===
{message}

=== ë‹µë³€ ì§€ì¹¨ ===
1. <think> íƒœê·¸ ì•ˆì—ì„œ ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •ì„ ìƒì„¸íˆ ê¸°ë¡í•˜ì„¸ìš”
2. ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…í™•íˆ í‘œì‹œí•˜ê³  ì¶”ê°€ í™•ì¸ì„ ê¶Œí•˜ì„¸ìš”
4. í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ë°˜ì˜í•œ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€:"""
            else:
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

<think>
ë‹¨ê³„ë³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
1. ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•…: {message}
2. ë„ë¡œê³µì‚¬ ê´€ë ¨ ì§€ì‹ í™œìš©
3. ì‹¤ë¬´ì  ë‹µë³€ êµ¬ì¡°í™”
4. ì¶”ê°€ ë„ì›€ ë°©ì•ˆ ì œì‹œ
</think>

ì‚¬ìš©ì ì§ˆë¬¸: {message}

ë„ë¡œ, êµí†µ, ê³ ì†ë„ë¡œì™€ ê´€ë ¨ëœ ì „ë¬¸ ì§€ì‹ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
        else:
            # ì¼ë°˜ ëª¨ë“œ í”„ë¡¬í”„íŠ¸
            if relevant_docs:
                context = "\n".join([
                    f"ğŸ“„ {doc['filename']} (ê´€ë ¨ë„: {doc['score']:.2f})\n{doc['content']}" 
                    for doc in relevant_docs
                ])
                
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì„œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ì‚¬ìš©ì ì§ˆë¬¸ ===
{message}

=== ë‹µë³€ ì§€ì¹¨ ===
1. ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë˜, ì¶”ì¸¡ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ì„±ì„ ì‚´ë ¤ ë‹µë³€í•˜ì„¸ìš”
4. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”

ë‹µë³€:"""
            else:
                prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {message}

í˜„ì¬ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì§€ë§Œ, ë„ë¡œ, êµí†µ, ê³ ì†ë„ë¡œì™€ ê´€ë ¨ëœ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
í•œêµ­ë„ë¡œê³µì‚¬ì˜ ì „ë¬¸ì„±ì„ ì‚´ë ¤ ì¹œê·¼í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
        
        # 3. ë¹ ë¥¸ ëª¨ë“œ ë˜ëŠ” ì¼ë°˜ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„±
        # standard ëª¨ë“œì¼ ë•Œ ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        if mode == 'think':
            ai_response = query_ollama_streaming(prompt)
        else:
            # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë¹ ë¥¸ ì‘ë‹µ
            simple_prompt = f"í•œêµ­ë„ë¡œê³µì‚¬ AIì…ë‹ˆë‹¤. ê°„ë‹¨íˆ ë‹µë³€í•˜ì„¸ìš”: {message}"
            ai_response = query_ollama_fast(simple_prompt)
        
        if not ai_response:
            if relevant_docs:
                ai_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ëª¨ë¸ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\n\ní•˜ì§€ë§Œ '{message}'ì™€ ê´€ë ¨ëœ ë¬¸ì„œ {len(relevant_docs)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ ì¶œì²˜ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”."
            else:
                ai_response = f"ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ë„ë¡œê³µì‚¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n'{message}'ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦¬ê³  ì‹¶ì§€ë§Œ, í˜„ì¬ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ê³  AI ëª¨ë¸ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # Think ëª¨ë“œ ì‘ë‹µ ì²˜ë¦¬
        thinking_process = None
        if mode == 'think' and ai_response:
            if "<think>" in ai_response and "</think>" in ai_response:
                start_idx = ai_response.find("<think>") + len("<think>")
                end_idx = ai_response.find("</think>")
                thinking_process = ai_response[start_idx:end_idx].strip()
                ai_response = ai_response[ai_response.find("</think>") + len("</think>"):].strip()
        
        processing_time = time.time() - start_time
        
        # 4. ì¶œì²˜ ì •ë³´ êµ¬ì„±
        sources = []
        if relevant_docs:
            sources = [
                {
                    "title": doc["filename"],
                    "content_preview": doc["content"],
                    "relevance_score": round(doc["score"], 3),
                    "page": doc.get("page", 1),
                    "document_type": doc.get("document_type", "ë¬¸ì„œ")
                }
                for doc in relevant_docs
            ]
        
        # 5. ë¡œê·¸ ê¸°ë¡
        log_request('í…ìŠ¤íŠ¸', user_id, f'ëª¨ë“œ: {mode}', 'success', f"{processing_time:.2f}ì´ˆ")
        
        response_data = {
            'reply': ai_response,
            'sources': sources,
            'status': 'success',
            'processing_time': f"{processing_time:.2f}ì´ˆ",
            'documents_found': len(relevant_docs),
            'mode': mode,
            'timestamp': datetime.now().isoformat()
        }
        
        # Think ëª¨ë“œì¸ ê²½ìš° ì‚¬ê³  ê³¼ì • ì¶”ê°€
        if thinking_process:
            response_data['thinking_process'] = thinking_process
        
        return jsonify(response_data)
        
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
        
        log_request('í…ìŠ¤íŠ¸', request.remote_addr, 'ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
        
        return jsonify({
            'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
            'status': 'error',
            'processing_time': f"{processing_time:.2f}ì´ˆ"
        }), 500

@app.route('/api/chat_stream', methods=['POST'])
def chat_stream():
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… API"""
    def generate():
        try:
            data = request.json
            message = data.get('message', '').strip()
            mode = data.get('mode', 'standard')
            
            # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
            if mode == 'think':
                prompt = f"<think>ìƒê°: {message}ì— ëŒ€í•´ ë¶„ì„</think>\ní•œêµ­ë„ë¡œê³µì‚¬ AIì…ë‹ˆë‹¤. {message}"
            else:
                prompt = f"í•œêµ­ë„ë¡œê³µì‚¬ AIì…ë‹ˆë‹¤. ê°„ë‹¨íˆ: {message}"
            
            # Ollama ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            payload = {
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 100,
                    "num_ctx": 256
                }
            }
            
            response = requests.post(OLLAMA_API_URL, json=payload, stream=True, timeout=30)
            
            for line in response.iter_lines():
                if line:
                    try:
                        json_line = json.loads(line.decode('utf-8'))
                        chunk = json_line.get("response", "")
                        if chunk:
                            yield f"data: {chunk}\n\n"
                        if json_line.get("done"):
                            break
                    except:
                        continue
                        
        except Exception as e:
            yield f"data: ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n"
    
    return Response(generate(), mimetype='text/plain')

@app.route('/api/admin/dashboard_data', methods=['GET'])
def get_dashboard_data():
    """ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ë°ì´í„°"""
    try:
        # GPU ìƒíƒœ
        gpu_info = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                device = f"cuda:{i}"
                memory_allocated = torch.cuda.memory_allocated(device)
                memory_total = torch.cuda.get_device_properties(device).total_memory
                
                gpu_info.append({
                    'device': device,
                    'memory_used_gb': memory_allocated / 1024**3,
                    'memory_total_gb': memory_total / 1024**3,
                    'utilization_percent': (memory_allocated / memory_total) * 100
                })
        
        # ì‹œìŠ¤í…œ í†µê³„
        total_requests = stats_data['total_requests']
        active_users = len(stats_data['active_users'])
        
        # ë¬¸ì„œ í†µê³„
        document_count = 0
        if qdrant_client:
            try:
                collection_info = qdrant_client.get_collection("documents")
                document_count = collection_info.points_count
            except:
                pass
        
        return jsonify({
            'total_requests': total_requests,
            'active_users': active_users,
            'gpu_usage': gpu_info,
            'document_count': document_count,
            'target_document_count': 2199,
            'system_uptime': "99.8%",
            'upload_sessions': len(upload_sessions),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/stats', methods=['GET'])
def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ë°ì´í„° ë°˜í™˜"""
    try:
        current_active = len(stats_data['active_users'])
        
        today = datetime.now()
        daily_data = {}
        for i in range(15):
            date = (today - timedelta(days=i)).strftime('%Y-%m-%d')
            daily_data[date] = stats_data['daily_usage'].get(date, 0)
        
        total = stats_data['total_requests']
        success_rate = (stats_data['successful_requests'] / total * 100) if total > 0 else 100
        
        return jsonify({
            'total_requests': stats_data['total_requests'],
            'successful_requests': stats_data['successful_requests'], 
            'failed_requests': stats_data['failed_requests'],
            'success_rate': f"{success_rate:.1f}%",
            'active_users': current_active,
            'daily_usage': daily_data,
            'recent_logs': stats_data['recent_logs'][:20],
            'gpu_usage_history': stats_data['gpu_usage_history'][-50:],
            'system_uptime': "99.7%",
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"í†µê³„ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        return jsonify({
            'status': 'healthy',
            'whisper_loaded': whisper_model is not None,
            'qdrant_connected': qdrant_client is not None,
            'embedding_model_loaded': embedding_model is not None,
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'total_requests': stats_data['total_requests'],
            'active_users': len(stats_data['active_users']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

@app.route('/api/qdrant_status', methods=['GET'])
def qdrant_status():
    """Qdrant ìƒíƒœ í™•ì¸ API"""
    try:
        if not qdrant_client:
            return jsonify({"status": "disconnected", "error": "í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"})
        
        collections = qdrant_client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if "documents" in collection_names:
            collection_info = qdrant_client.get_collection("documents")
            return jsonify({
                "status": "connected",
                "collection_exists": True,
                "points_count": collection_info.points_count,
                "vectors_count": collection_info.vectors_count
            })
        else:
            return jsonify({
                "status": "connected",
                "collection_exists": False,
                "error": "documents ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            })
            
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

@app.route('/api/test', methods=['GET']) 
def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        "message": "EX-GPT ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.", 
        "whisper_loaded": whisper_model is not None,
        "qdrant_connected": qdrant_client is not None,
        "embedding_loaded": embedding_model is not None,
        "endpoints": [
            "/api/chat",
            "/api/stats",
            "/api/health",
            "/api/test"
        ]
    })

# ì˜¤ë¥˜ í•¸ë“¤ëŸ¬
@app.route('/favicon.ico')
def favicon():
    return '', 204

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Endpoint not found'}), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(error)}")
    return jsonify({'error': 'Internal server error'}), 500

@app.route('/images/<path:filename>')
def serve_images(filename):
    """ì´ë¯¸ì§€ íŒŒì¼ ì„œë¹„ìŠ¤"""
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì§€ì •
        image_directory = '/mnt/c/projects/ex-gpt-demo/images'
        return send_from_directory(image_directory, filename)
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ ì˜¤ë¥˜ ({filename}): {e}")
        return jsonify({'error': f'Image not found: {filename}'}), 404
 
@app.route('/api/upload_voice', methods=['POST'])
def upload_voice():
    """ìŒì„± íŒŒì¼ ì—…ë¡œë“œ ë° STT ì²˜ë¦¬"""
    start_time = time.time()
    session_id = None
    
    try:
        # íŒŒì¼ ê²€ì¦
        if 'audio' not in request.files:
            return jsonify({'error': 'ìŒì„± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        file = request.files['audio']
        if file.filename == '':
            return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        if not allowed_audio_file(file.filename):
            return jsonify({'error': f'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.'}), 400
        
        user_id = request.form.get('user_id', request.remote_addr)
        process_type = request.form.get('type', 'transcribe')
        
        # ì²˜ë¦¬ ì„¸ì…˜ ìƒì„±
        session_id = create_processing_session(user_id)
        
        logger.info(f"ğŸ¤ ìŒì„± íŒŒì¼ ì—…ë¡œë“œ: {file.filename} (ì„¸ì…˜: {session_id})")
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_dir = tempfile.mkdtemp()
        filename = secure_filename(file.filename)
        temp_file_path = os.path.join(temp_dir, filename)
        file.save(temp_file_path)
        
        update_processing_session(session_id, 'uploading', 20, 'íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ')
        
        try:
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
            update_processing_session(session_id, 'preprocessing', 40, 'ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì¤‘...')
            processed_audio_path = preprocess_audio(temp_file_path)
            
            # STT ìˆ˜í–‰
            update_processing_session(session_id, 'transcribing', 60, 'ìŒì„± ì¸ì‹ ì¤‘...')
            transcription_result = transcribe_audio(processed_audio_path)
            
            if not transcription_result:
                raise Exception('STT ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
            
            # í›„ì²˜ë¦¬
            update_processing_session(session_id, 'processing', 80, f'{process_type} ì²˜ë¦¬ ì¤‘...')
            processed_result = process_transcription(transcription_result, process_type)
            
            processing_time = time.time() - start_time
            
            # ê²°ê³¼ êµ¬ì„±
            final_result = {
                'status': 'success',
                'transcription': transcription_result,
                'processed_content': processed_result,
                'processing_time': f"{processing_time:.2f}ì´ˆ",
                'session_id': session_id,
                'timestamp': datetime.now().isoformat()
            }
            
            update_processing_session(session_id, 'completed', 100, 'ì²˜ë¦¬ ì™„ë£Œ', final_result)
            
            # ë¡œê·¸ ê¸°ë¡
            log_request('ìŒì„±', user_id, f'STT-{process_type}', 'success', f"{processing_time:.2f}ì´ˆ")
            
            return jsonify(final_result)
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                if os.path.exists(temp_file_path):
                    os.remove(temp_file_path)
                if processed_audio_path and os.path.exists(processed_audio_path):
                    os.remove(processed_audio_path)
                if os.path.exists(temp_dir):
                    os.rmdir(temp_dir)
            except Exception as e:
                logger.warning(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
    except Exception as e:
        processing_time = time.time() - start_time
        error_message = f'ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        
        logger.error(f"âŒ ìŒì„± ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
        if session_id:
            update_processing_session(session_id, 'error', 0, error_message)
        
        log_request('ìŒì„±', request.remote_addr, 'STT ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
        
        return jsonify({
            'error': error_message,
            'status': 'error',
            'processing_time': f"{processing_time:.2f}ì´ˆ",
            'session_id': session_id
        }), 500

@app.route('/api/audio_progress/<session_id>', methods=['GET'])
def get_audio_progress(session_id):
    """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì§„í–‰ë¥  ì¡°íšŒ"""
    session = audio_processing_sessions.get(session_id)
    if not session:
        return jsonify({'error': 'ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
    
    return jsonify({
        'session_id': session_id,
        'status': session['status'],
        'progress': session['progress'],
        'message': session['message'],
        'timestamp': session.get('updated_at', session['created_at']).isoformat()
    })
    
def preprocess_audio(audio_path):
    """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ì œê±°, í¬ë§· ë³€í™˜ ë“±)"""
    try:
        logger.info("ğŸ”§ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = AudioSegment.from_file(audio_path)
        
        # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
        if audio.channels > 1:
            audio = audio.set_channels(1)
        
        # ìƒ˜í”Œë ˆì´íŠ¸ 16kHzë¡œ ë³€í™˜ (Whisper ìµœì í™”)
        audio = audio.set_frame_rate(16000)
        
        # ë³¼ë¥¨ ì •ê·œí™”
        audio = audio.normalize()
        
        # ë¬´ìŒ ì œê±°
        chunks = split_on_silence(
            audio,
            min_silence_len=500,  # 0.5ì´ˆ ì´ìƒ ë¬´ìŒ
            silence_thresh=audio.dBFS - 16,
            keep_silence=250  # 0.25ì´ˆ ë¬´ìŒ ìœ ì§€
        )
        
        if chunks:
            audio = sum(chunks)
        
        # ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì €ì¥
        processed_path = audio_path.replace('.', '_processed.')
        if not processed_path.endswith('.wav'):
            processed_path = processed_path.rsplit('.', 1)[0] + '.wav'
        
        audio.export(processed_path, format="wav")
        
        # ë…¸ì´ì¦ˆ ë¦¬ë•ì…˜ (ì„ íƒì )
        try:
            import librosa
            y, sr = librosa.load(processed_path, sr=16000)
            y_reduced = nr.reduce_noise(y=y, sr=sr, prop_decrease=0.8)
            
            # ë‹¤ì‹œ ì €ì¥
            import soundfile as sf
            sf.write(processed_path, y_reduced, sr)
            
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ë¦¬ë•ì…˜ ì‹¤íŒ¨: {e}")
        
        logger.info("âœ… ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì™„ë£Œ")
        return processed_path
        
    except Exception as e:
        logger.error(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return audio_path  # ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

# Flask ì•± ì‹œì‘ ì‹œ ëª¨ë¸ ì´ˆê¸°í™”
def initialize_app_with_audio():
    """Flask ì•± ì‹œì‘ ì‹œ í˜¸ì¶œí•  ì´ˆê¸°í™” í•¨ìˆ˜"""
    logger.info("ğŸ¤ ìŒì„± ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
    
    # CUDA í™˜ê²½ í™•ì¸
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        logger.info(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    # ì˜¤ë””ì˜¤ ëª¨ë¸ë“¤ ì´ˆê¸°í™”
    success = initialize_audio_models()
    
    if success:
        logger.info("ğŸ‰ ìŒì„± ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
    else:
        logger.error("âŒ ìŒì„± ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨!")
        return False

# ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜
def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

logger.info("ğŸ¤ CUDA í˜¸í™˜ ìŒì„± ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")

def transcribe_audio(audio_path):
    """Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹"""
    try:
        if not whisper_model:
            raise Exception("Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logger.info("ğŸ¯ Whisper STT ì‹¤í–‰...")
        
        # Whisper ì˜µì…˜ ì„¤ì •
        options = {
            "language": "ko",  # í•œêµ­ì–´ ìš°ì„ 
            "task": "transcribe",
            "fp16": torch.cuda.is_available(),  # GPU ì‚¬ìš© ì‹œ FP16
            "temperature": 0,  # ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´
            "best_of": 2,
            "beam_size": 5,
            "patience": 1.0,
            "length_penalty": -0.05,
            "suppress_tokens": "-1",
            "initial_prompt": "ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ë„ë¡œê³µì‚¬ì…ë‹ˆë‹¤."  # ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸
        }
        
        # STT ìˆ˜í–‰
        result = whisper_model.transcribe(audio_path, **options)
        
        # ê²°ê³¼ ê²€ì¦
        text = result.get("text", "").strip()
        if not text:
            raise Exception("ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì–¸ì–´ ê°ì§€ ê²°ê³¼ ë¡œê¹…
        detected_language = result.get("language", "unknown")
        logger.info(f"ğŸŒ ê°ì§€ëœ ì–¸ì–´: {detected_language}")
        
        if detected_language != "ko" and detected_language != "korean":
            logger.warning(f"âš ï¸ í•œêµ­ì–´ê°€ ì•„ë‹Œ ì–¸ì–´ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤: {detected_language}")
        
        logger.info(f"âœ… STT ì™„ë£Œ: {len(text)}ì")
        return text
        
    except Exception as e:
        logger.error(f"STT ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def process_transcription(text, process_type):
    """STT ê²°ê³¼ í›„ì²˜ë¦¬"""
    try:
        if process_type == "transcribe":
            # ë‹¨ìˆœ ì „ì‚¬
            return format_transcription(text)
            
        elif process_type == "summarize":
            # ìš”ì•½ ì²˜ë¦¬
            return summarize_text(text)
            
        elif process_type == "analyze":
            # ë¶„ì„ ì²˜ë¦¬
            return analyze_speech_content(text)
            
        else:
            return format_transcription(text)
            
    except Exception as e:
        logger.error(f"í›„ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return format_transcription(text)

def format_transcription(text):
    """ì „ì‚¬ í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
    # ê¸°ë³¸ í¬ë§·íŒ…
    formatted = text.strip()
    
    # ë¬¸ì¥ ë¶€í˜¸ ì¶”ê°€
    if formatted and not formatted.endswith(('.', '!', '?')):
        formatted += '.'
    
    # ì¤„ë°”ê¿ˆ ì²˜ë¦¬
    sentences = formatted.split('. ')
    if len(sentences) > 3:
        formatted = '. '.join(sentences[:len(sentences)//2]) + '.\n\n' + '. '.join(sentences[len(sentences)//2:])
    
    return formatted

def summarize_text(text):
    """í…ìŠ¤íŠ¸ ìš”ì•½"""
    try:
        # Ollamaë¥¼ ì‚¬ìš©í•œ ìš”ì•½
        prompt = f"""ë‹¤ìŒ ìŒì„± ì „ì‚¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì›ë³¸ ë‚´ìš©:
{text}

ìš”ì•½ ì§€ì¹¨:
1. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ë‹¨íˆ ì •ë¦¬
2. ì¤‘ìš”í•œ í‚¤ì›Œë“œ í¬í•¨
3. 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
4. í•œêµ­ë„ë¡œê³µì‚¬ ì—…ë¬´ì™€ ê´€ë ¨ëœ ë‚´ìš©ì´ë©´ ë” ìì„¸íˆ

ìš”ì•½:"""

        summary = query_ollama_fast(prompt)
        if summary:
            return f"ğŸ“ **ìš”ì•½**\n\n{summary}\n\n---\n\nğŸ“„ **ì›ë³¸ ì „ì‚¬**\n\n{format_transcription(text)}"
        else:
            return format_transcription(text)
            
    except Exception as e:
        logger.error(f"ìš”ì•½ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return format_transcription(text)

def analyze_speech_content(text):
    """ìŒì„± ë‚´ìš© ë¶„ì„"""
    try:
        # ë¶„ì„ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¤ìŒ ìŒì„± ì „ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì „ì‚¬ ë‚´ìš©:
{text}

ë¶„ì„ í•­ëª©:
1. ì£¼ìš” ì£¼ì œ
2. í•µì‹¬ í‚¤ì›Œë“œ
3. ê°ì •/í†¤
4. ì•¡ì…˜ ì•„ì´í…œ (ìˆë‹¤ë©´)
5. í•œêµ­ë„ë¡œê³µì‚¬ ì—…ë¬´ ê´€ë ¨ì„±

ë¶„ì„ ê²°ê³¼:"""

        analysis = query_ollama_fast(prompt)
        if analysis:
            return f"ğŸ” **ìŒì„± ë‚´ìš© ë¶„ì„**\n\n{analysis}\n\n---\n\nğŸ“„ **ì›ë³¸ ì „ì‚¬**\n\n{format_transcription(text)}"
        else:
            return format_transcription(text)
            
    except Exception as e:
        logger.error(f"ë¶„ì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return format_transcription(text)

def get_audio_duration(audio_path):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ ë°˜í™˜"""
    try:
        audio = AudioSegment.from_file(audio_path)
        return round(len(audio) / 1000.0, 1)  # ì´ˆ ë‹¨ìœ„
    except:
        return 0.0

# ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬ë¥¼ ìœ„í•œ WebSocket ì§€ì› (ì„ íƒì‚¬í•­)
@app.route('/api/voice_stream', methods=['POST'])
def voice_stream():
    """ì‹¤ì‹œê°„ ìŒì„± ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬"""
    try:
        # ì‹¤ì‹œê°„ ìŒì„± ë°ì´í„° ì²˜ë¦¬
        # êµ¬í˜„ ì‹œ WebSocketì´ë‚˜ Server-Sent Events ì‚¬ìš© ê¶Œì¥
        return jsonify({"message": "ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬ëŠ” WebSocketìœ¼ë¡œ êµ¬í˜„ ì˜ˆì •"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
        
# ì£¼ê¸°ì  ì •ë¦¬ ì‘ì—…
def cleanup_old_data():
    """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
    if len(stats_data['recent_logs']) > 1000:
        stats_data['recent_logs'] = stats_data['recent_logs'][:500]
    
    if len(stats_data['gpu_usage_history']) > 2000:
        stats_data['gpu_usage_history'] = stats_data['gpu_usage_history'][-1000:]
    
    # í™œì„± ì‚¬ìš©ì ëª©ë¡ ì •ë¦¬
    stats_data['active_users'].clear()

def periodic_cleanup():
    """ì£¼ê¸°ì  ì •ë¦¬ ì‹¤í–‰"""
    while True:
        time.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤
        cleanup_old_data()

# ============================================================================
# STEP 5: ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ìˆ˜ì •
# ============================================================================

# ê¸°ì¡´ ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ì„ ì°¾ì•„ì„œ ìˆ˜ì •í•˜ì„¸ìš”:

if __name__ == '__main__':
    logger.info("ğŸš€ EX-GPT ì„œë²„ ì‹œì‘...")
    
    # 1. CUDA í™˜ê²½ ê²€ì¦
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
    else:
        logger.warning("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # 2. Qdrant ì´ˆê¸°í™”
    logger.info("ğŸ”— Qdrant ì´ˆê¸°í™”...")
    qdrant_client = initialize_qdrant()
    if qdrant_client:
        logger.info("âœ… Qdrant ì¤€ë¹„ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ Qdrant ì´ˆê¸°í™” ì‹¤íŒ¨. ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

    # 3. ìŒì„± ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
    logger.info("ğŸ¤ ìŒì„± ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™”...")
    audio_success = initialize_audio_models()
    if audio_success:
        logger.info("âœ… ìŒì„± ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ ìŒì„± ì²˜ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨. STT ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    
    # 4. ì •ë¦¬ ì‘ì—… ì‹œì‘
    cleanup_thread = threading.Thread(target=periodic_cleanup, daemon=True)
    cleanup_thread.start()
    
    logger.info("ğŸ‰ ëª¨ë“  ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    logger.info("ğŸ“ ì„œë²„ ì£¼ì†Œ: http://localhost:5001")
    logger.info("ğŸ“Š ê´€ë¦¬ì: http://localhost:5001/admin")
    logger.info("ğŸ” ê¸°ëŠ¥:")
    logger.info("   - âœ… ì‹¤ì‹œê°„ Ollama LLM ì—°ë™")
    logger.info("   - âœ… Qdrant ë¬¸ì„œ ê²€ìƒ‰")
    logger.info("   - âœ… ìŒì„± ì¸ì‹ ë° ìš”ì•½")  # ìƒˆë¡œ ì¶”ê°€
    
    app.run(host='0.0.0.0', port=5001, debug=False, threaded=True)