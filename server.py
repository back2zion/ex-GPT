#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import warnings
warnings.filterwarnings("ignore", message="pkg_resources is deprecated")

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import requests
import json
import logging
import time
from datetime import datetime
import os
import re

# ============= ë¡œê¹… ì„¤ì • =============
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============= Flask ì•± ë° CORS ì„¤ì • =============
app = Flask(__name__)
CORS(app)

# RAG API ì„¤ì • (Docker ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜)
NEOALI_API_URL = "http://localhost:8081/v1/chat/"

# í†µê³„ ë°ì´í„°
stats_data = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'active_users': set(),
}

# ëŒ€í™” ì´ë ¥ ì €ì¥ (ì„¸ì…˜ë³„)
conversation_history = {}

# ë™ì  íŒ¨í„´ í•™ìŠµ ë°ì´í„°
pattern_learning_data = {
    'misrouted_queries': [],
    'user_feedback': [],
    'pattern_performance': {},
    'last_update': datetime.now()
}

# íŒ¨í„´ ì»´íŒŒì¼ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)
compiled_patterns_cache = {}

def determine_routing_path(message, mode):
    """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì§€ëŠ¥í˜• ì§ˆì˜ ë¼ìš°íŒ… ì‹œìŠ¤í…œ"""
    
    # 1. ìµœê³  ìš°ì„ ìˆœìœ„: ì¦‰ì‹œ ì°¨ë‹¨ íŒ¨í„´ (ë³´ì•ˆ/ê°œì¸ì •ë³´)
    security_patterns = [
        r'(ë¹„ë°€ë²ˆí˜¸|íŒ¨ìŠ¤ì›Œë“œ|ê°œì¸ì •ë³´|ì£¼ë¯¼ë²ˆí˜¸|ì‹ ìš©ì¹´ë“œ|ê³„ì¢Œë²ˆí˜¸)',
        r'(í•´í‚¹|í¬ë™|ë¶ˆë²•|ìœ„ì¡°|ë³€ì¡°)'
    ]
    
    # 2. ê³ ìš°ì„ ìˆœìœ„: í•œêµ­ë„ë¡œê³µì‚¬ í•µì‹¬ ì—…ë¬´ (ê°€ì¤‘ì¹˜ 5)
    high_priority_rag_patterns = [
        r'(í†µí–‰ë£Œ|ë¶€ê°€í†µí–‰ë£Œ|ê¸°ë³¸ìš”ê¸ˆ|ì£¼í–‰ìš”ê¸ˆ|ê±°ë¦¬ë¹„ë¡€|ìš”ê¸ˆì²´ê³„)',
        r'(í• ì¸|ê°ë©´|ë©´ì œ|ì •ê¸°ê¶Œ|íšŒìˆ˜ê¶Œ|ì‹¬ì•¼í• ì¸|ê²½ì°¨í• ì¸)',
        r'(í•œêµ­ë„ë¡œê³µì‚¬|ë„ë¡œê³µì‚¬|ë„ê³µ|KEC)',
        r'(ê·œì •|ì§€ì¹¨|ë§¤ë‰´ì–¼|ì ˆì°¨|ê¸°ì¤€|í‘œì¤€|ë°©ì¹¨)',
        r'(í•˜ì´íŒ¨ìŠ¤|Hi-pass|ì „ìê²°ì œ|ì„ ë¶ˆ|í›„ë¶ˆ)'
    ]
    
    # 3. ì¤‘ìš°ì„ ìˆœìœ„: ë„ë¡œ ì‹œì„¤/ìš´ì˜ (ê°€ì¤‘ì¹˜ 3)
    medium_priority_rag_patterns = [
        r'(ê³ ì†ë„ë¡œ|ê³ ì†êµ­ë„|ìë™ì°¨ì „ìš©ë„ë¡œ|í†¨ê²Œì´íŠ¸|ìš”ê¸ˆì†Œ)',
        r'(ë‚˜ë“¤ëª©|ì¸í„°ì²´ì¸ì§€|IC|ë¶„ê¸°ì |ì •ì…˜|JC)',
        r'(íœ´ê²Œì†Œ|ì¡¸ìŒì‰¼í„°|ë§Œë‚¨ì˜ê´‘ì¥|ì£¼ì°¨ì¥)',
        r'(í„°ë„|êµëŸ‰|ê³ ê°€|ì§€í•˜ì°¨ë„|ìœ¡êµ)',
        r'(êµí†µì •ë³´|ì†Œí†µìƒí™©|ì •ì²´|ì§€ì²´|ìš°íšŒë„ë¡œ)'
    ]
    
    # 4. ì €ìš°ì„ ìˆœìœ„: ì¼ë°˜ ëŒ€í™” (ê°€ì¤‘ì¹˜ 1)
    direct_llm_patterns = [
        r'^(ì•ˆë…•|í•˜ì´|í—¬ë¡œ|hi|hello)',
        r'^(ê°ì‚¬|ê³ ë§ˆì›Œ|ìˆ˜ê³ )',
        r'^(ë„¤|ì˜ˆ|ì•„ë‹ˆ|ê·¸ë˜|ë§ì•„)',
        r'^\d+\s*[+\-*/]\s*\d+\s*=?',  # ë‹¨ìˆœ ê³„ì‚°
        r'(ë‚ ì”¨|ê¸°ì˜¨|ì˜¨ë„|ë¹„|ëˆˆ|ë°”ëŒ)',
        r'(ìŒì‹|ìš”ë¦¬|ë§›ì§‘|ì‹ë‹¹)',
        r'(ì˜í™”|ë“œë¼ë§ˆ|ìŒì•…|ê²Œì„)',
        r'(ìŠ¤í¬ì¸ |ì¶•êµ¬|ì•¼êµ¬|ë†êµ¬)'
    ]
    
    # 2. í•œêµ­ë„ë¡œê³µì‚¬ ì „ë¬¸ìš©ì–´ RAG ê²€ìƒ‰ íŒ¨í„´ (ëŒ€í­ í™•ì¥)
    rag_search_patterns = [
        # ê¸°ë³¸ ë„ë¡œê³µì‚¬ ê´€ë ¨
        r'í•œêµ­ë„ë¡œê³µì‚¬|ë„ë¡œê³µì‚¬|ë„ê³µ|KEC|Korea\s*Expressway',
        
        # ê³ ì†ë„ë¡œ ì‹œì„¤ ë° êµ¬ì¡°ë¬¼
        r'ê³ ì†ë„ë¡œ|ê³ ì†êµ­ë„|ìë™ì°¨ì „ìš©ë„ë¡œ|í†¨ê²Œì´íŠ¸|ìš”ê¸ˆì†Œ|í•˜ì´íŒ¨ìŠ¤|Hi-pass',
        r'ë‚˜ë“¤ëª©|ì¸í„°ì²´ì¸ì§€|IC|ë¶„ê¸°ì |ì •ì…˜|JC|êµì°¨ë¡œ|ë¨í”„',
        r'íœ´ê²Œì†Œ|ì¡¸ìŒì‰¼í„°|ë§Œë‚¨ì˜ê´‘ì¥|ì£¼ì°¨ì¥|í™”ë¬¼ì°¨íœ´ê²Œì†Œ|LPGì¶©ì „ì†Œ',
        r'í„°ë„|êµëŸ‰|ê³ ê°€|ì§€í•˜ì°¨ë„|ìœ¡êµ|ê³¼ì„ êµ|ê±´ë„ëª©',
        r'ì°¨ë¡œ|ì£¼í–‰ì°¨ë¡œ|ì¶”ì›”ì°¨ë¡œ|ê°“ê¸¸|ì¤‘ì•™ë¶„ë¦¬ëŒ€|ë°©ìŒë²½|ê°€ë“œë ˆì¼',
        
        # í†µí–‰ë£Œ ë° ìš”ê¸ˆ ì²´ê³„
        r'í†µí–‰ë£Œ|ë¶€ê°€í†µí–‰ë£Œ|ê¸°ë³¸ìš”ê¸ˆ|ì£¼í–‰ìš”ê¸ˆ|ê±°ë¦¬ë¹„ë¡€|ìš”ê¸ˆì²´ê³„',
        r'í• ì¸|ê°ë©´|ë©´ì œ|ì •ê¸°ê¶Œ|íšŒìˆ˜ê¶Œ|ì‹¬ì•¼í• ì¸|ê²½ì°¨í• ì¸|ì´ìš©ë¹ˆë„í• ì¸',
        r'í•˜ì´íŒ¨ìŠ¤ì¹´ë“œ|êµí†µì¹´ë“œ|ì „ìê²°ì œ|ì„ ë¶ˆ|í›„ë¶ˆ|ì¶©ì „',
        r'í†µí–‰ë£Œìˆ˜ì…|ìš”ê¸ˆì§•ìˆ˜|ë¯¸ë‚©|ì²´ë‚©|í™˜ë¶ˆ|ì •ì‚°',
        r'ìš”ê¸ˆí‘œ|ìš”ê¸ˆì‚°ì •|í• ì¸ìœ¨|ê²½ë¡œë³„ìš”ê¸ˆ',
        
        # êµí†µ ìš´ì˜ ë° ê´€ë¦¬
        r'êµí†µì •ë³´|ì†Œí†µìƒí™©|ì •ì²´|ì§€ì²´|ìš°íšŒë„ë¡œ|í†µì œ',
        r'VMS|ì „ê´‘íŒ|CCTV|ITS|í•˜ì´íŒ¨ìŠ¤ì‹œìŠ¤í…œ|ETC',
        r'êµí†µëŸ‰|êµí†µíë¦„|í†µí–‰ì†ë„|í˜¼ì¡ë„|LOS',
        r'ì‚¬ê³ |ëŒë°œìƒí™©|ì‘ê¸‰ìƒí™©|ê²¬ì¸|êµ¬ë‚œ|ì œì„¤|ì œë¹™',
        r'ë„ë¡œìˆœì°°|ì ê²€|ì •ë¹„|ë³´ìˆ˜|êµì²´|ê°œëŸ‰|í™•ì¥',
        
        # ì¡°ì§ ë° ë¶€ì„œ
        r'ë³¸ë¶€|ì‚¬ì—…ë³¸ë¶€|ê±´ì„¤ì‚¬ì—…ë³¸ë¶€|ìš´ì˜ì‚¬ì—…ë³¸ë¶€|ë””ì§€í„¸í˜ì‹ ë³¸ë¶€',
        r'ì²˜|ì‹¤|ë¶€|íŒ€|ì„¼í„°|ì‚¬ë¬´ì†Œ|ì§€ì‚¬|ê´€ë¦¬ì†Œ|ì‚¬ì—…ë‹¨',
        r'ì¸ë ¥ì²˜|ì¬ë¬´ì²˜|ê¸°íšì²˜|ë””ì§€í„¸ê³„íšì²˜|AIë°ì´í„°ë¶€|ìš´ì˜ë¶€',
        
        # ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ ë° ê·œì •
        r'ê·œì •|ì§€ì¹¨|ë§¤ë‰´ì–¼|ì ˆì°¨|ê¸°ì¤€|í‘œì¤€|ë°©ì¹¨|ì§€ì‹œ|ê³µì§€',
        r'ë²•ë ¹|ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™|ì¡°ë¡€|ë‚´ê·œ|ì—…ë¬´ê·œì •|ê´€ë¦¬ê·œì •',
        r'ìŠ¹ì¸|ê²°ì¬|ì‹ ì²­|í—ˆê°€|ì¸ê°€|ë“±ë¡|ì‹ ê³ |ë³´ê³ ',
        r'ì¸ì¥|ë„ì¥|ì„œëª…|ë‚ ì¸|ì§ì¸|ê³„ì•½ì„œ|í˜‘ì •ì„œ|í•©ì˜ì„œ',
        
        # ì¸ì‚¬ ë° ê¸‰ì—¬
        r'ì¸ì‚¬|ë°œë ¹|ì „ë³´|ìŠ¹ì§„|ì±„ìš©|í‡´ì§|íœ´ì§|ë³µì§',
        r'ê¸‰ì—¬|ìˆ˜ë‹¹|ìƒì—¬|ì„±ê³¼ê¸‰|ë³µë¦¬í›„ìƒ|í‡´ì§ê¸ˆ|ì—°ê¸ˆ',
        r'ê·¼ë¬´|ì¶œê·¼|í‡´ê·¼|ì—°ê°€|ë³‘ê°€|íœ´ê°€|íŠ¹ë³„íœ´ê°€|ìœ¡ì•„íœ´ì§',
        r'êµìœ¡|ì—°ìˆ˜|í›ˆë ¨|ì›Œí¬ìˆ|ì„¸ë¯¸ë‚˜|ì»¨í¼ëŸ°ìŠ¤|ìê²©ì¦',
        
        # ì¬ë¬´ ë° íšŒê³„
        r'ì˜ˆì‚°|ê²°ì‚°|íšŒê³„|ìê¸ˆ|íˆ¬ì|ìœµì|ëŒ€ì¶œ|ë³´ì¦',
        r'ë²•ì¸ì¹´ë“œ|ì¶œì¥ë¹„|ì—¬ë¹„|êµí†µë¹„|ìˆ™ë°•ë¹„|ì‹ë¹„|ì¼ë¹„',
        r'ê²½ë¹„|ë¹„ìš©|ì§€ì¶œ|ìˆ˜ì…|ë§¤ì¶œ|ì†ìµ|ìì‚°|ë¶€ì±„',
        r'ì¡°ë‹¬|êµ¬ë§¤|ì…ì°°|ê³„ì•½|ê²€ìˆ˜|ê²€ì‚¬|ë‚©í’ˆ|ëŒ€ê¸ˆì§€ê¸‰',
        
        # ê±´ì„¤ ë° ê³µì‚¬
        r'ê±´ì„¤|ì‹ ì„¤|í™•ì¥|ê°œëŸ‰|ë³´ê°•|ì •ë¹„|ìœ ì§€ê´€ë¦¬',
        r'ê³µì‚¬|ê³µì •|ê³µì¢…|ì‹œê³µ|ì¤€ê³µ|ì¤€ê³µê²€ì‚¬|ì¤€ê³µë„ì„œ',
        r'ì„¤ê³„|ê°ë¦¬|ê°ë…|ê²€ì¸¡|í’ˆì§ˆê´€ë¦¬|ì•ˆì „ê´€ë¦¬',
        
        # ì•ˆì „ ë° í™˜ê²½
        r'ì•ˆì „|ì‚¬ê³ |ì¬í•´|ìœ„í—˜|ì ê²€|ì§„ë‹¨|í‰ê°€|ëŒ€ì±…',
        r'ì•ˆì „êµìœ¡|ì•ˆì „ì ê²€|ì•ˆì „ê´€ë¦¬|ì•ˆì „ì‹œì„¤|ì•ˆì „ì¥ë¹„',
        r'í™˜ê²½|ì¹œí™˜ê²½|ë…¹ìƒ‰|íƒ„ì†Œ|ë°°ì¶œ|ì†ŒìŒ|ì§„ë™|ëŒ€ê¸°ì§ˆ',
        
        # ì‹œê°„ ë° ë‚ ì§œ ê´€ë ¨
        r'2024ë…„|2025ë…„|2023ë…„|ì˜¬í•´|ì‘ë…„|ë‚´ë…„|ê¸ˆë…„|ë…„ë„',
        r'ì›”|ì¼|ì‹œ|ë¶„|ê¸°ê°„|ê¸°í•œ|ë§ˆê°|ì—°ì¥|ë‹¨ì¶•',
        r'í‰ì¼|ì£¼ë§|ê³µíœ´ì¼|íœ´ì¼|ê·¼ë¬´ì‹œê°„|ì—…ë¬´ì‹œê°„',
        
        # ìˆ˜ì¹˜ ë° í†µê³„
        r'ëª‡|ì–¼ë§ˆ|ìˆ˜ì¹˜|ë¹„ìœ¨|í¼ì„¼íŠ¸|ê°œìˆ˜|ëª…ìˆ˜|ëŒ€ìˆ˜',
        r'í†µê³„|ë¶„ì„|í˜„í™©|ì‹¤ì |ì„±ê³¼|ì§€í‘œ|ëª©í‘œ|ë‹¬ì„±',
        r'ì›|ì–µ|ì²œë§Œ|ë°±ë§Œ|ë§Œ|ì²œ|ë°±|ì‹­',
        r'km|m|cm|í†¤|kg|ëŒ€|ê°œ|ëª…|ê±´|íšŒ|ì°¨ë¡€',
        
        # ìœ„ì¹˜ ë° ì§€ì—­
        r'ì–´ë””|ìœ„ì¹˜|ì¥ì†Œ|ì§€ì—­|êµ¬ê°„|ë…¸ì„ |êµ¬ê°„ë³„',
        r'ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…',
        r'ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼',
        
        # ì°¨ëŸ‰ ë° ìš´ì†¡
        r'ì°¨ëŸ‰|ìë™ì°¨|ìŠ¹ìš©ì°¨|í™”ë¬¼ì°¨|ë²„ìŠ¤|ì˜¤í† ë°”ì´|ì´ë¥œì°¨',
        r'ê²½ì°¨|ì†Œí˜•ì°¨|ì¤‘í˜•ì°¨|ëŒ€í˜•ì°¨|íŠ¹ìˆ˜ì°¨|ê±´ì„¤ê¸°ê³„',
        r'ì „ê¸°ì°¨|í•˜ì´ë¸Œë¦¬ë“œ|ìˆ˜ì†Œì°¨|ì¹œí™˜ê²½ì°¨|ë¬´ê³µí•´ì°¨'
    ]
    
    # 3. query_expansion ê²½ë¡œ íŒ¨í„´
    expansion_patterns = [
        r'^(ê·¸ê±°|ì´ê±°|ì €ê±°|ê·¸ê²ƒ|ì´ê²ƒ|ì €ê²ƒ)\s*(ë­|ì–´ë–»ê²Œ|ì–¸ì œ)',
        r'(ì¶œì²˜|ì–´ë””ì„œ|ì–´ë–»ê²Œ)\s*(í™•ì¸|ì°¾|ì•Œ)',
        r'(ë”|ì¢€ë”|ìì„¸íˆ|êµ¬ì²´ì ìœ¼ë¡œ)\s*(ì•Œê³ |ì„¤ëª…|ì„¤ëª…í•´)',
        r'(ê´€ë ¨|ëŒ€í•œ|ì—\s*ëŒ€í•´)\s*(ë‚´ìš©|ì •ë³´|ìë£Œ)',
        r'^(ë­|ë­”ê°€|ë¬´ì—‡|ì–´ë–¤)\s*(ìˆ|í•˜|ë¨)',
        r'^(ë„¤|ì˜ˆ|ì•„ë‹ˆ|ê·¸ëŸ¼)\s*',
        r'^[ê°€-í£]{1,3}$'  # ë§¤ìš° ì§§ì€ í•œê¸€
    ]
    
    # 4. mcp_action ê²½ë¡œ íŒ¨í„´
    mcp_action_patterns = [
        r'íŒŒì¼|ì—…ë¡œë“œ|ì²¨ë¶€|ë‹¤ìš´ë¡œë“œ|ì €ì¥',
        r'ìŒì„±|ë…¹ìŒ|ë³€í™˜|í…ìŠ¤íŠ¸|ë³€ê²½',
        r'íšŒì˜|íšŒì˜ë¡|ìš”ì•½|ì •ë¦¬|ê¸°ë¡',
        r'ë²ˆì—­|ì˜ì–´|ì¤‘êµ­ì–´|ì¼ë³¸ì–´|translate',
        r'ë¹„êµ|ëŒ€ì¡°|ë¶„ì„|ê²€í† |í‰ê°€',
        r'ì´ë¯¸ì§€|ì‚¬ì§„|ê·¸ë¦¼|ë„í‘œ|ì°¨íŠ¸'
    ]
    
    import re
    
    message_lower = message.lower()
    
    # 1ë‹¨ê³„: ë³´ì•ˆ íŒ¨í„´ ìš°ì„  ê²€ì‚¬
    for pattern in security_patterns:
        if re.search(pattern, message_lower):
            return {
                'path': 'direct_llm',
                'confidence': 'high', 
                'reasoning': 'ë³´ì•ˆ/ê°œì¸ì •ë³´ ê´€ë ¨ ì§ˆì˜ëŠ” ë‹µë³€ ë¶ˆê°€',
                'scores': {'security': 1}
            }
    
    # 2ë‹¨ê³„: ìš°ì„ ìˆœìœ„ë³„ ê°€ì¤‘ì¹˜ ì ìˆ˜ ê³„ì‚°
    scores = {
        'direct_llm': 0,
        'rag_search': 0, 
        'query_expansion': 0,
        'mcp_action': 0
    }
    
    # ê³ ìš°ì„ ìˆœìœ„ íŒ¨í„´ ê²€ì‚¬ (ê°€ì¤‘ì¹˜ 5)
    for pattern in high_priority_rag_patterns:
        if re.search(pattern, message_lower):
            scores['rag_search'] += 5
            
    # ì¤‘ìš°ì„ ìˆœìœ„ íŒ¨í„´ ê²€ì‚¬ (ê°€ì¤‘ì¹˜ 3)  
    for pattern in medium_priority_rag_patterns:
        if re.search(pattern, message_lower):
            scores['rag_search'] += 3
            
    # ì €ìš°ì„ ìˆœìœ„ ì¼ë°˜ ëŒ€í™” íŒ¨í„´ (ê°€ì¤‘ì¹˜ 1)
    for pattern in direct_llm_patterns:
        if re.search(pattern, message_lower):
            scores['direct_llm'] += 1
    
    # ë³µí•© ì§ˆì˜ ì²˜ë¦¬: "í•˜ì´íŒ¨ìŠ¤ í• ì¸ ê·œì •" ê°™ì€ ê²½ìš°
    question_indicators = ['ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ì–¼ë§ˆ', 'ëª‡', 'ì•Œë ¤ì¤˜', 'ì°¾ì•„ì¤˜', 'í™•ì¸']
    for indicator in question_indicators:
        if indicator in message:
            scores['rag_search'] += 2  # ì§ˆë¬¸ í˜•íƒœë©´ ë¬¸ì„œ ê²€ìƒ‰ ìš°ì„ 
            break
    
    # ëª¨ë“œë³„ ê°€ì¤‘ì¹˜ ì ìš©
    if mode == 'search':
        scores['rag_search'] += 3
    
    # ë©”ì‹œì§€ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
    if len(message) < 5:
        scores['query_expansion'] += 1
    elif len(message) > 30:
        scores['rag_search'] += 1
    
    # ìµœê³  ì ìˆ˜ ê²½ë¡œ ì„ íƒ
    best_path = max(scores, key=scores.get)
    max_score = scores[best_path]
    
    # ì‹ ë¢°ë„ ê³„ì‚° (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
    if max_score >= 5:
        confidence = 'high'
    elif max_score >= 3:
        confidence = 'medium'
    elif max_score >= 1:
        confidence = 'low'
    else:
        # ê¸°ë³¸ê°’: í•œêµ­ë„ë¡œê³µì‚¬ AIì´ë¯€ë¡œ RAG ìš°ì„ 
        best_path = 'rag_search'
        confidence = 'medium'
    
    # ì¶”ë¡  ì´ìœ  ìƒì„±
    reasoning_map = {
        'direct_llm': 'ì¼ë°˜ ì§€ì‹/ì¸ì‚¬ ì§ˆì˜',
        'rag_search': 'ë„ë¡œê³µì‚¬ ë¬¸ì„œ ê²€ìƒ‰',
        'query_expansion': 'ì§ˆì˜ í™•ì¥ í•„ìš”',
        'mcp_action': 'íŠ¹ìˆ˜ ê¸°ëŠ¥ ìš”ì²­'
    }
    
    # íŒ¨í„´ ì„±ëŠ¥ ì¶”ì 
    pattern_key = f"{best_path}_{confidence}"
    if pattern_key not in pattern_learning_data['pattern_performance']:
        pattern_learning_data['pattern_performance'][pattern_key] = {'count': 0, 'success': 0}
    pattern_learning_data['pattern_performance'][pattern_key]['count'] += 1
    
    return {
        'path': best_path,
        'confidence': confidence,
        'reasoning': reasoning_map[best_path],
        'scores': scores,
        'pattern_key': pattern_key
    }

def expand_query(message):
    """ì§ˆì˜ í™•ì¥ í•¨ìˆ˜"""
    
    # ëª¨í˜¸í•œ ì§ˆì˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¥
    expansion_rules = {
        r'^ê·¸ê±°|ì €ê±°|ì´ê±°': 'ì•ì„œ ì–¸ê¸‰í•œ ë‚´ìš©ê³¼ ê´€ë ¨í•˜ì—¬ í•œêµ­ë„ë¡œê³µì‚¬ì˜',
        r'^ë”|ì¶”ê°€': message + ' ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ì™€ ê´€ë ¨ ê·œì •',
        r'^ì¶œì²˜': message.replace('ì¶œì²˜', '') + ' ì— ëŒ€í•œ ê·¼ê±° ë¬¸ì„œì™€ ì¶œì²˜',
        r'ê´€ë ¨': message + ' ì™€ ì—°ê´€ëœ ëª¨ë“  ì •ë³´',
        r'^ì™œ|ì´ìœ ': message + ' ì˜ ë°°ê²½ê³¼ ì´ìœ  ë° ê´€ë ¨ ì •ì±…'
    }
    
    import re
    for pattern, expansion in expansion_rules.items():
        if re.search(pattern, message):
            return expansion
    
    # ê¸°ë³¸ í™•ì¥: í•œêµ­ë„ë¡œê³µì‚¬ ë§¥ë½ ì¶”ê°€
    return f"í•œêµ­ë„ë¡œê³µì‚¬ì™€ ê´€ë ¨ëœ {message} ì— ëŒ€í•œ ì •ë³´"

def update_pattern_performance(pattern_key, success=True):
    """íŒ¨í„´ ì„±ëŠ¥ ì—…ë°ì´íŠ¸"""
    if pattern_key in pattern_learning_data['pattern_performance']:
        if success:
            pattern_learning_data['pattern_performance'][pattern_key]['success'] += 1

def collect_misrouted_query(query, expected_path, actual_path):
    """ì˜ëª» ë¼ìš°íŒ…ëœ ì§ˆì˜ ìˆ˜ì§‘"""
    misrouted_data = {
        'query': query,
        'expected': expected_path,
        'actual': actual_path,
        'timestamp': datetime.now()
    }
    pattern_learning_data['misrouted_queries'].append(misrouted_data)
    
    # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
    if len(pattern_learning_data['misrouted_queries']) > 100:
        pattern_learning_data['misrouted_queries'] = pattern_learning_data['misrouted_queries'][-100:]

def suggest_new_patterns():
    """ìƒˆë¡œìš´ íŒ¨í„´ ì œì•ˆ í•¨ìˆ˜"""
    suggestions = []
    
    # ìì£¼ ì˜ëª» ë¼ìš°íŒ…ë˜ëŠ” í‚¤ì›Œë“œ ë¶„ì„
    misrouted_queries = pattern_learning_data['misrouted_queries']
    if len(misrouted_queries) < 5:
        return suggestions
    
    # ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ
    from collections import Counter
    all_words = []
    for item in misrouted_queries[-20:]:  # ìµœê·¼ 20ê°œ ë¶„ì„
        words = item['query'].split()
        all_words.extend([w for w in words if len(w) >= 2])
    
    common_words = Counter(all_words).most_common(10)
    
    for word, count in common_words:
        if count >= 3:  # 3ë²ˆ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´
            suggestions.append({
                'pattern': f'({word})',
                'frequency': count,
                'suggested_path': 'rag_search',
                'reason': f'ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ: {word} ({count}íšŒ)'
            })
    
    return suggestions

def auto_update_patterns():
    """ìë™ íŒ¨í„´ ì—…ë°ì´íŠ¸ (ì£¼ê¸°ì  ì‹¤í–‰)"""
    global pattern_learning_data
    
    # 1ì£¼ì¼ë§ˆë‹¤ íŒ¨í„´ ì—…ë°ì´íŠ¸
    time_diff = datetime.now() - pattern_learning_data['last_update']
    if time_diff.days < 7:
        return False
    
    suggestions = suggest_new_patterns()
    
    # ì„±ëŠ¥ì´ ì¢‹ì€ ì œì•ˆë§Œ ìë™ ì ìš©
    for suggestion in suggestions:
        if suggestion['frequency'] >= 5:  # 5íšŒ ì´ìƒ ë“±ì¥ì‹œ ìë™ ì ìš©
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ íŒ¨í„´ ìë™ ì¶”ê°€: {suggestion['pattern']}")
            # ì‹¤ì œ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì—¬ê¸°ì„œëŠ” ë¡œê·¸ë§Œ)
    
    pattern_learning_data['last_update'] = datetime.now()
    return True

def expand_query_with_context(message, context):
    """ë§¥ë½ì„ ê³ ë ¤í•œ ì§ˆì˜ í™•ì¥ í•¨ìˆ˜"""
    
    # ì´ì „ ëŒ€í™”ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = context.get('previous_topic', [])
    recent_messages = context.get('recent_messages', [])
    
    import re
    
    # ì°¸ì¡° í‘œí˜„ ì²˜ë¦¬
    if re.search(r'^ê·¸ê±°|ì €ê±°|ì´ê±°|ê·¸ê²ƒ|ì´ê²ƒ', message):
        if keywords:
            main_topic = ' '.join(keywords[:3])  # ì£¼ìš” í‚¤ì›Œë“œ 3ê°œ
            return f"{main_topic}ê³¼ ê´€ë ¨í•˜ì—¬ {message.replace('ê·¸ê±°', '').replace('ì €ê±°', '').replace('ì´ê±°', '').strip()}"
        elif recent_messages:
            return f"ì•ì„œ ì–¸ê¸‰í•œ '{recent_messages[-1][:20]}...'ê³¼ ê´€ë ¨í•˜ì—¬ ì¶”ê°€ ì •ë³´"
    
    # "ë”", "ì¶”ê°€" ë“±ì˜ í™•ì¥ ìš”ì²­
    if re.search(r'^ë”|ì¶”ê°€|ìì„¸íˆ', message):
        if keywords:
            main_topic = ' '.join(keywords[:2])
            return f"{main_topic}ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ì™€ ê´€ë ¨ ê·œì • ë° ì ˆì°¨"
    
    # "ì™œ", "ì´ìœ " ë“±ì˜ ë°°ê²½ ì§ˆë¬¸
    if re.search(r'^ì™œ|ì´ìœ |ë°°ê²½', message):
        if keywords:
            main_topic = ' '.join(keywords[:2])
            return f"{main_topic}ì˜ ë°°ê²½ê³¼ ì´ìœ , ê´€ë ¨ ì •ì±… ë° ë²•ì  ê·¼ê±°"
    
    # ê¸°ë³¸ ë§¥ë½ í™•ì¥
    if keywords:
        context_str = ' '.join(keywords[:3])
        return f"{context_str}ì™€ ê´€ë ¨ëœ {message} ì— ëŒ€í•œ ì •ë³´"
    
    return expand_query(message)

def get_conversation_history(session_id, max_turns=10):
    """ëŒ€í™” ì´ë ¥ ì¡°íšŒ"""
    if session_id not in conversation_history:
        conversation_history[session_id] = []
    
    # ìµœê·¼ max_turnsë§Œ ë°˜í™˜
    return conversation_history[session_id][-max_turns:]

def filter_chinese_characters(text):
    """ì¤‘êµ­ì–´ ë¬¸ì í•„í„°ë§ í•¨ìˆ˜"""
    chinese_chars = [
        "å¦‚", "æ‚¨", "ç­‰", "ä¼š", "ç½‘ç«™", "æˆ–", "ç§»åŠ¨", "åº”ç”¨", "å®¢æœ", "å¯ä»¥", "ç›´æ¥", "æŸ¥è¯¢", 
        "æœ€æ–°", "ä¿¡æ¯", "æˆ‘ä»¬", "æä¾›", "å¤šå°‘", "è´¹ç”¨", "éœ€è¦", "æ”¿ç­–", "å› ä¸º", "å¯èƒ½", "æ ¹æ®", 
        "åœ°åŒº", "å…·ä½“", "æƒ…å†µ", "ä¸åŒ", "å»ºè®®", "é€šè¿‡", "å®˜æ–¹", "è¿›è¡Œ", "è”ç³»", "è·å–", "å‡†ç¡®",
        "à¹„à¸¡", "åŒ…æ‹¬", "ä»»ä½•", "ç¦æ­¢", "çš„", "è¯­è¨€", "å­—ç¬¦", "ä¸æ˜¯", "è®¨è®º", "å†…å®¹", "è¯·", "ç»§ç»­",
        "ä½¿ç”¨", "äº¤æµ", "å°†", "æŒ‰ç…§", "è¦æ±‚", "å¸®åŠ©", "ä»Šå¤©", "æƒ³", "äº†è§£", "å“ªäº›", "å…³äº",
        "æ—¥å¸¸", "æ„Ÿè°¢", "æµç¨‹", "å‘¢", "é’ˆå¯¹", "ç‰¹å®š", "åœºåˆ", "è¯¦ç»†", "æ²»ç–—", "ç†è§£", "é›†è¡Œ",
        "ä¸»ç®¡", "éƒ¨é—¨", "æŒ‡å®š", "ç®¡ç†è€…", "å½±å“", "ä¸¥æ ¼", "ê·€", "Ä±rÄ±m", "í•´ê²°", "ì•ˆë˜ê³ ", "ìˆìŒ",
        "hoáº·c", "ê´€ë ¨", "ìš´ì˜", "ìƒí™©", "ì—°ê´€", "ê°€ì¥", "ì •í™•í•œ", "ì–»ê¸°", "í™•ì¸"
    ]
    
    filtered_text = text
    found_chinese = []
    
    for char in chinese_chars:
        if char in filtered_text:
            found_chinese.append(char)
            filtered_text = filtered_text.replace(char, "")
    
    if found_chinese:
        logger.warning(f"ì¤‘êµ­ì–´/ì™¸êµ­ì–´ ë¬¸ì ê°ì§€ ë° ì œê±°: {found_chinese}")
        
        # ì˜ë¯¸ê°€ ì‹¬í•˜ê²Œ ê¹¨ì§„ ë¬¸ì¥ì´ë©´ í•œêµ­ì–´ ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
        if len(filtered_text.strip()) < len(text.strip()) * 0.6:  # 40% ì´ìƒ ì œê±°ë˜ì—ˆìœ¼ë©´
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ í•œêµ­ë„ë¡œê³µì‚¬ ê³ ê°ì„¼í„°(1588-2504)ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
    
    return filtered_text.strip()

def add_to_conversation_history(session_id, user_message, assistant_message):
    """ëŒ€í™” ì´ë ¥ì— ì¶”ê°€ (ì¤‘êµ­ì–´ í•„í„°ë§ í¬í•¨)"""
    if session_id not in conversation_history:
        conversation_history[session_id] = []
    
    # ì‘ë‹µì—ì„œ ì¤‘êµ­ì–´/ì™¸êµ­ì–´ í•„í„°ë§
    cleaned_response = filter_chinese_characters(assistant_message)
    
    conversation_history[session_id].append({
        'role': 'user',
        'content': user_message,
        'timestamp': datetime.now().isoformat()
    })
    
    conversation_history[session_id].append({
        'role': 'assistant', 
        'content': cleaned_response,
        'timestamp': datetime.now().isoformat()
    })
    
    # ìµœëŒ€ 20í„´(40ê°œ ë©”ì‹œì§€)ê¹Œì§€ë§Œ ìœ ì§€
    if len(conversation_history[session_id]) > 40:
        conversation_history[session_id] = conversation_history[session_id][-40:]
    
    return cleaned_response

def analyze_conversation_context(session_id, current_message):
    """ëŒ€í™” ë§¥ë½ ë¶„ì„"""
    history = get_conversation_history(session_id)
    
    if not history:
        return {
            'has_context': False,
            'previous_topic': None,
            'needs_expansion': False
        }
    
    # ìµœê·¼ ë©”ì‹œì§€ë“¤ ë¶„ì„
    recent_messages = [msg['content'] for msg in history[-4:]]
    context_keywords = []
    
    for msg in recent_messages:
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        words = msg.split()
        context_keywords.extend([word for word in words if len(word) > 2])
    
    # í˜„ì¬ ë©”ì‹œì§€ê°€ ë§¥ë½ ì°¸ì¡°ì¸ì§€ í™•ì¸
    reference_patterns = [r'^ê·¸ê±°|ì €ê±°|ì´ê±°|ê·¸ê²ƒ|ì´ê²ƒ', r'^ë”|ì¶”ê°€', r'^ì™œ|ì´ìœ ', r'^ì–´ë–»ê²Œ']
    needs_expansion = any(re.search(pattern, current_message) for pattern in reference_patterns)
    
    return {
        'has_context': len(history) > 0,
        'previous_topic': context_keywords[-5:] if context_keywords else None,
        'needs_expansion': needs_expansion,
        'recent_messages': recent_messages[-2:] if recent_messages else []
    }

def log_request(request_type, user_id, action, status, processing_time):
    """ìš”ì²­ ë¡œê·¸ ê¸°ë¡"""
    stats_data['total_requests'] += 1
    if status == 'success':
        stats_data['successful_requests'] += 1
    else:
        stats_data['failed_requests'] += 1
    
    logger.info(f"{request_type} - {user_id} - {action} - {status} - {processing_time}")

# =============================================================================
# ë¼ìš°íŠ¸ ì •ì˜
# =============================================================================

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€ - ì—…ë°ì´íŠ¸ëœ UI ì œê³µ"""
    try:
        return send_from_directory('.', 'index.html')
    except Exception as e:
        logger.error(f"ë©”ì¸ í˜ì´ì§€ ì˜¤ë¥˜: {str(e)}")
        return f"<h1>ex-GPT ì„œë²„</h1><p>index.html íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}</p>", 500

@app.route('/api/chat', methods=['POST'])
def chat_proxy():
    """RAG RAG APIë¡œ í”„ë¡ì‹œí•˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    start_time = time.time()
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹± ì‹œë„
        try:
            data = request.get_json(force=True)
        except Exception as json_error:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(json_error)}")
            try:
                # raw dataë¡œ ì‹œë„
                raw_data = request.get_data(as_text=True)
                logger.info(f"Raw ìš”ì²­ ë°ì´í„°: {raw_data}")
                data = json.loads(raw_data)
            except Exception as raw_error:
                logger.error(f"Raw ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {str(raw_error)}")
                return jsonify({'error': f'ìš”ì²­ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {str(json_error)}'}), 400
        
        if not data:
            return jsonify({'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ ëª¨ë“œ ë° ì„¸ì…˜ ì •ë³´ í™•ì¸
        message = data.get('message', '')
        mode = data.get('mode', 'general')
        force_rag = data.get('force_rag', False)
        session_id = data.get('session_id', request.remote_addr)  # ì„¸ì…˜ ID (ê¸°ë³¸ê°’: IP)
        
        # ëŒ€í™” ë§¥ë½ ë¶„ì„
        context = analyze_conversation_context(session_id, message)
        logger.info(f"ğŸ” ëŒ€í™” ë§¥ë½: {context}")
        
        # ë§¥ë½ ê¸°ë°˜ ë¼ìš°íŒ… ê²°ì •
        routing_result = determine_routing_path(message, mode)
        route_path = routing_result['path']
        confidence = routing_result['confidence']
        reasoning = routing_result['reasoning']
        
        # ë§¥ë½ì´ ìˆëŠ” ê²½ìš° query_expansion ìš°ì„  ê³ ë ¤
        if context['needs_expansion'] and context['has_context']:
            route_path = 'query_expansion'
            confidence = 'high'
            reasoning = 'ëŒ€í™” ë§¥ë½ ì°¸ì¡° ì§ˆì˜'
            logger.info(f"ğŸ”„ ë§¥ë½ ê¸°ë°˜ ë¼ìš°íŒ… ì¡°ì •: query_expansion")
        
        logger.info(f"ğŸ“ ìµœì¢… ë¼ìš°íŒ… ê²°ì •: {route_path} (ì‹ ë¢°ë„: {confidence}, ì´ìœ : {reasoning})")
        
        # ëŒ€í™” ì´ë ¥ êµ¬ì„±
        history = get_conversation_history(session_id)
        
        # ë¼ìš°íŒ… ê²½ë¡œë³„ ì²˜ë¦¬
        if route_path == 'rag_search':
            # RAG ê²€ìƒ‰ ê²½ë¡œ - ëŒ€í™” ì´ë ¥ í¬í•¨
            full_history = history + [{
                "role": "user",
                "content": message
            }]
            neoali_payload = {
                "history": full_history,
                "stream": False,
                "search_documents": True,
                "return_documents": True,
                "include_default_system_prompt": True
            }
            use_rag = True
        elif route_path == 'query_expansion':
            # ì§ˆì˜ í™•ì¥ ê²½ë¡œ - ë§¥ë½ì„ ê³ ë ¤í•œ í™•ì¥
            if context['has_context']:
                expanded_query = expand_query_with_context(message, context)
            else:
                expanded_query = expand_query(message)
            
            full_history = history + [{
                "role": "user",
                "content": expanded_query
            }]
            neoali_payload = {
                "history": full_history,
                "stream": False,
                "search_documents": True,
                "return_documents": True,
                "include_default_system_prompt": True
            }
            use_rag = True
        elif route_path == 'mcp_action':
            # íŠ¹ìˆ˜ ê¸°ëŠ¥ ê²½ë¡œ - í˜„ì¬ëŠ” ì¼ë°˜ LLMìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  í–¥í›„ í™•ì¥
            neoali_payload = None
            use_rag = False
        else:
            # direct_llm ê²½ë¡œ - ëŒ€í™” ì´ë ¥ í¬í•¨
            neoali_payload = None
            use_rag = False
        
        user_id = data.get('user_id', request.remote_addr)
        stats_data['active_users'].add(user_id)
        
        if use_rag and neoali_payload:
            logger.info(f"ğŸ’¬ ì±„íŒ… ìš”ì²­ -> RAG RAG API (í•œêµ­ë„ë¡œê³µì‚¬ ë¬¸ì„œ ê²€ìƒ‰)")
            
            # RAG RAG API í˜¸ì¶œ ì‹œë„
            try:
                response = requests.post(
                    NEOALI_API_URL,
                    json=neoali_payload,
                    headers={
                        'Content-Type': 'application/json',
                        'Accept': 'application/json'
                    },
                    timeout=30
                )
            except requests.exceptions.ConnectionError:
                # RAG API ì—°ê²° ì‹¤íŒ¨ì‹œ ì§ì ‘ vLLM í˜¸ì¶œ
                logger.warning("RAG API ì—°ê²° ì‹¤íŒ¨, vLLM ì§ì ‘ í˜¸ì¶œë¡œ ì „í™˜")
                use_rag = False
        
        if not use_rag:
            logger.info(f"ğŸ’¬ ì¼ë°˜ ëŒ€í™” ìš”ì²­ -> vLLM ì§ì ‘ í˜¸ì¶œ")
            
            # ëŒ€í™” ì´ë ¥ì„ vLLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            system_prompt = """You are ex-GPT, an AI assistant for Korea Expressway Corporation (í•œêµ­ë„ë¡œê³µì‚¬).

## Core Identity
- Role: Professional Korean expressway expert assistant
- Organization: Korea Expressway Corporation (KEC)
- Language: ALWAYS respond in Korean only

## Language Policy (CRITICAL)
```python
ALLOWED_LANGUAGES = ["korean"]
FORBIDDEN_LANGUAGES = ["chinese", "english", "japanese"]
FORBIDDEN_CHARS = ["å¦‚", "æ‚¨", "ç­‰", "ä¼š", "ç½‘ç«™", "æˆ–", "ç§»åŠ¨", "åº”ç”¨", "å®¢æœ", "å¯ä»¥", "ç›´æ¥", "æŸ¥è¯¢", "æœ€æ–°", "ä¿¡æ¯", "æˆ‘ä»¬", "æä¾›", "å¤šå°‘", "è´¹ç”¨", "éœ€è¦", "æ”¿ç­–", "å› ä¸º", "å¯èƒ½", "æ ¹æ®", "åœ°åŒº", "å…·ä½“", "æƒ…å†µ", "ä¸åŒ", "å»ºè®®", "é€šè¿‡", "å®˜æ–¹", "è¿›è¡Œ", "è”ç³»", "è·å–", "å‡†ç¡®"]

def validate_response(text):
    return not any(char in text for char in FORBIDDEN_CHARS)
```

## Memory & Context
- Remember user names and conversation history
- When asked about names, recall accurately
- Maintain conversation context across turns

## Response Templates (Markdown Format)
```python
GREETING_TEMPLATE = "## ì•ˆë…•í•˜ì„¸ìš”, **{name}ë‹˜**! ğŸ‘‹\n\ní•œêµ­ë„ë¡œê³µì‚¬ `ex-GPT`ì…ë‹ˆë‹¤."
HELP_TEMPLATE = "> ğŸ’¡ **ë„ì›€ë§**: í•œêµ­ë„ë¡œê³µì‚¬ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."
PROCEDURE_TEMPLATE = "## {title}\\n\\n### ğŸ“‹ ì ˆì°¨ ê°œìš”\\n{overview}\\n\\n### ğŸ“ ë‹¨ê³„ë³„ ì•ˆë‚´\\n{steps}\\n\\n### âš ï¸ ì£¼ì˜ì‚¬í•­\\n{notes}"
```

## Output Rules
1. ONLY Korean language in responses
2. NO Chinese/English/Japanese characters
3. ALWAYS use Markdown formatting - THIS IS MANDATORY:
   - **Bold** for important points and emphasis
   - ## Headers for sections and topics
   - ### Sub-headers for detailed breakdown
   - - Lists for multiple items and procedures
   - 1. Numbered lists for step-by-step processes
   - `code` for technical terms, amounts, percentages
   - > Blockquotes for important notes and warnings
   - | Tables | for | structured data |
4. Professional and helpful tone
5. Remember conversation context
6. Structure responses with clear sections using headers"""
            messages = [{"role": "system", "content": system_prompt}]
            
            # ëŒ€í™” ì´ë ¥ ì¶”ê°€ (ìµœê·¼ 5í„´ë§Œ)
            for hist_msg in history[-10:]:  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ (5í„´)
                messages.append({
                    "role": hist_msg["role"],
                    "content": hist_msg["content"]
                })
            
            # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": message})
            
            vllm_payload = {
                "model": "default-model",
                "messages": messages,
                "stream": False
            }
            
            try:
                response = requests.post(
                    "http://localhost:8000/v1/chat/completions",
                    json=vllm_payload,
                    headers={
                        'Content-Type': 'application/json',
                        'Authorization': 'Bearer dummy-key'
                    },
                    timeout=30
                )
            except requests.exceptions.ConnectionError:
                # vLLM ì—°ê²° ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ì‘ë‹µ
                fallback_response = {
                    'reply': f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì´ ì‹œì‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {message}\n\ní•œêµ­ë„ë¡œê³µì‚¬ ê´€ë ¨ ë¬¸ì˜ëŠ” ê³ ê°ì„¼í„°(1588-2504)ë¡œ ì—°ë½í•˜ì‹œê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì´ìš©í•´ì£¼ì„¸ìš”.",
                    'sources': [],
                    'status': 'AI ëª¨ë¸ ë¡œë”© ì¤‘',
                    'processing_time': f"0.01ì´ˆ",
                    'timestamp': datetime.now().isoformat()
                }
                log_request('ì±„íŒ…', user_id, 'ì—°ê²° ì˜¤ë¥˜', 'error', '0.00ì´ˆ')
                return jsonify(fallback_response)
        
        processing_time = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            
            # RAGê°€ "Connection error" ì‘ë‹µí•˜ëŠ” ê²½ìš° vLLM ì§ì ‘ í˜¸ì¶œ
            if 'detail' in result and 'Connection error' in str(result.get('detail', '')):
                logger.warning("RAG ë‚´ë¶€ ì—°ê²° ì˜¤ë¥˜, vLLM ì§ì ‘ í˜¸ì¶œë¡œ ì „í™˜")
                
                korean_only_prompt = """ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ ex-GPTì…ë‹ˆë‹¤.

CRITICAL LANGUAGE RULES - MUST FOLLOW:
1. ì˜¤ì§ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. 
2. ì¤‘êµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€
3. ì¤‘êµ­ì–´ ë¬¸ì(å¦‚, æ‚¨, ç­‰, ä¼š, é²·é±¼, æ¬¢è¿) ì‚¬ìš© ê¸ˆì§€
4. ì˜ì–´ ë‹¨ì–´ ìµœì†Œí™”, í•œêµ­ì–´ ìš°ì„ 
5. ëŒ€í™” ìƒëŒ€ë°©ì˜ ì´ë¦„ì„ ì •í™•íˆ ê¸°ì–µí•˜ì„¸ìš”

CONVERSATION CONTEXT:
- ì‚¬ìš©ì ì´ë¦„ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì„¸ìš”
- ì´ë¦„ì„ ë¬¼ìœ¼ë©´ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”

RESPONSE FORMAT:
- ì¸ì‚¬: "ì•ˆë…•í•˜ì„¸ìš”, [ì´ë¦„]ë‹˜!"
- ë„ì›€ ì œì•ˆ: "í•œêµ­ë„ë¡œê³µì‚¬ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."
- í•œêµ­ì–´ë§Œ ì‚¬ìš©

ì ˆëŒ€ ì¤‘êµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!"""
                
                vllm_payload = {
                    "model": "default-model",
                    "messages": [
                        {"role": "system", "content": korean_only_prompt},
                        {"role": "user", "content": message}
                    ],
                    "stream": False
                }
                
                try:
                    vllm_response = requests.post(
                        "http://localhost:8000/v1/chat/completions",
                        json=vllm_payload,
                        headers={
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer dummy-key'
                        },
                        timeout=30
                    )
                except requests.exceptions.ConnectionError:
                    # vLLM ì—°ê²° ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ì‘ë‹µ
                    fallback_response = {
                        'reply': f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì´ ì‹œì‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {message}\n\ní•œêµ­ë„ë¡œê³µì‚¬ ê´€ë ¨ ë¬¸ì˜ëŠ” ê³ ê°ì„¼í„°(1588-2504)ë¡œ ì—°ë½í•˜ì‹œê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì´ìš©í•´ì£¼ì„¸ìš”.",
                        'sources': [],
                        'status': 'AI ëª¨ë¸ ë¡œë”© ì¤‘',
                        'processing_time': f"{processing_time:.2f}ì´ˆ",
                        'timestamp': datetime.now().isoformat()
                    }
                    log_request('ì±„íŒ…', user_id, 'ì—°ê²° ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
                    return jsonify(fallback_response)
                
                if vllm_response.status_code == 200:
                    vllm_result = vllm_response.json()
                    content = vllm_result['choices'][0]['message']['content']
                    formatted_response = {
                        'reply': content,
                        'sources': [],
                        'status': 'success (vLLM ì§ì ‘ í˜¸ì¶œ)',
                        'route_path': route_path,
                        'confidence': confidence,
                        'reasoning': reasoning,
                        'processing_time': f"{processing_time:.2f}ì´ˆ",
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€ (ìë™ í•„í„°ë§)
                    cleaned_content = add_to_conversation_history(session_id, message, content)
                    formatted_response['reply'] = cleaned_content  # í•„í„°ë§ëœ ì‘ë‹µìœ¼ë¡œ ì—…ë°ì´íŠ¸
                    log_request('ì±„íŒ…', user_id, 'vLLM ì§ì ‘ ì—°ë™', 'success', f"{processing_time:.2f}ì´ˆ")
                    return jsonify(formatted_response)
            
            # ì‘ë‹µ í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬
            if 'response' in result:
                # RAG í˜•ì‹: {"response": "...", "sources": [...]}
                formatted_response = {
                    'reply': result['response'],
                    'sources': result.get('sources', []),
                    'status': f'success ({route_path})',
                    'route_path': route_path,
                    'confidence': confidence,
                    'reasoning': reasoning,
                    'processing_time': f"{processing_time:.2f}ì´ˆ",
                    'timestamp': datetime.now().isoformat()
                }
                
                # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€ (ìë™ í•„í„°ë§)
                cleaned_response = add_to_conversation_history(session_id, message, result['response'])
                formatted_response['reply'] = cleaned_response  # í•„í„°ë§ëœ ì‘ë‹µìœ¼ë¡œ ì—…ë°ì´íŠ¸
                log_request('ì±„íŒ…', user_id, 'RAG ê²€ìƒ‰', 'success', f"{processing_time:.2f}ì´ˆ")
            elif 'choices' in result:
                # vLLM í˜•ì‹: {"choices": [{"message": {"content": "..."}}]}
                content = result['choices'][0]['message']['content']
                formatted_response = {
                    'reply': content,
                    'sources': [],
                    'status': f'success ({route_path})',
                    'route_path': route_path,
                    'confidence': confidence,
                    'reasoning': reasoning,
                    'processing_time': f"{processing_time:.2f}ì´ˆ",
                    'timestamp': datetime.now().isoformat()
                }
                
                # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€ (ìë™ í•„í„°ë§)
                cleaned_content = add_to_conversation_history(session_id, message, content)
                formatted_response['reply'] = cleaned_content  # í•„í„°ë§ëœ ì‘ë‹µìœ¼ë¡œ ì—…ë°ì´íŠ¸
                log_request('ì±„íŒ…', user_id, 'ì¼ë°˜ ëŒ€í™”', 'success', f"{processing_time:.2f}ì´ˆ")
            else:
                formatted_response = result
                log_request('ì±„íŒ…', user_id, 'ê¸°íƒ€', 'success', f"{processing_time:.2f}ì´ˆ")
            
            return jsonify(formatted_response)
            
        else:
            # 500 ì˜¤ë¥˜ì—ì„œ Connection errorì¸ ê²½ìš° vLLM ì§ì ‘ í˜¸ì¶œ
            if response.status_code == 500:
                try:
                    error_detail = response.json().get('detail', '')
                    if 'Connection error' in str(error_detail):
                        logger.warning("RAG 500 ì˜¤ë¥˜ (Connection error), vLLM ì§ì ‘ í˜¸ì¶œë¡œ ì „í™˜")
                        
                        korean_strict_prompt = """You are ex-GPT, an AI assistant for Korea Expressway Corporation (í•œêµ­ë„ë¡œê³µì‚¬).

## Core Identity
- Role: Professional Korean expressway expert assistant
- Organization: Korea Expressway Corporation (KEC)
- Language: ALWAYS respond in Korean only

## Language Policy (CRITICAL)
```python
ALLOWED_LANGUAGES = ["korean"]
FORBIDDEN_LANGUAGES = ["chinese", "english", "japanese"]
FORBIDDEN_CHARS = ["å¦‚", "æ‚¨", "ç­‰", "ä¼š", "ç½‘ç«™", "æˆ–", "ç§»åŠ¨", "åº”ç”¨", "å®¢æœ", "å¯ä»¥", "ç›´æ¥", "æŸ¥è¯¢", "æœ€æ–°", "ä¿¡æ¯", "æˆ‘ä»¬", "æä¾›", "å¤šå°‘", "è´¹ç”¨", "éœ€è¦", "æ”¿ç­–", "å› ä¸º", "å¯èƒ½", "æ ¹æ®", "åœ°åŒº", "å…·ä½“", "æƒ…å†µ", "ä¸åŒ", "å»ºè®®", "é€šè¿‡", "å®˜æ–¹", "è¿›è¡Œ", "è”ç³»", "è·å–", "å‡†ç¡®"]

def validate_response(text):
    return not any(char in text for char in FORBIDDEN_CHARS)
```

## Memory & Context
- Remember user names and conversation history
- When asked about names, recall accurately
- Maintain conversation context across turns

## Response Templates (Markdown Format)
```python
GREETING_TEMPLATE = "## ì•ˆë…•í•˜ì„¸ìš”, **{name}ë‹˜**! ğŸ‘‹\n\ní•œêµ­ë„ë¡œê³µì‚¬ `ex-GPT`ì…ë‹ˆë‹¤."
HELP_TEMPLATE = "> ğŸ’¡ **ë„ì›€ë§**: í•œêµ­ë„ë¡œê³µì‚¬ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."
PROCEDURE_TEMPLATE = "## {title}\\n\\n### ğŸ“‹ ì ˆì°¨ ê°œìš”\\n{overview}\\n\\n### ğŸ“ ë‹¨ê³„ë³„ ì•ˆë‚´\\n{steps}\\n\\n### âš ï¸ ì£¼ì˜ì‚¬í•­\\n{notes}"
```

## Output Rules
1. ONLY Korean language in responses
2. NO Chinese/English/Japanese characters
3. ALWAYS use Markdown formatting - THIS IS MANDATORY:
   - **Bold** for important points and emphasis
   - ## Headers for sections and topics
   - ### Sub-headers for detailed breakdown
   - - Lists for multiple items and procedures
   - 1. Numbered lists for step-by-step processes
   - `code` for technical terms, amounts, percentages
   - > Blockquotes for important notes and warnings
   - | Tables | for | structured data |
4. Professional and helpful tone
5. Remember conversation context
6. Structure responses with clear sections using headers"""

                        # ëŒ€í™” ì´ë ¥ì„ í¬í•¨í•œ ë©”ì‹œì§€ êµ¬ì„±
                        vllm_messages = [{"role": "system", "content": korean_strict_prompt}]
                        
                        # ëŒ€í™” ì´ë ¥ ì¶”ê°€ (ìµœê·¼ 5í„´ë§Œ)
                        for hist_msg in history[-10:]:
                            vllm_messages.append({
                                "role": hist_msg["role"],
                                "content": hist_msg["content"]
                            })
                        
                        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
                        vllm_messages.append({"role": "user", "content": message})
                        
                        vllm_payload = {
                            "model": "default-model", 
                            "messages": vllm_messages,
                            "stream": False
                        }
                        
                        try:
                            vllm_response = requests.post(
                                "http://localhost:8000/v1/chat/completions",
                                json=vllm_payload,
                                headers={
                                    'Content-Type': 'application/json',
                                    'Authorization': 'Bearer dummy-key'
                                },
                                timeout=30
                            )
                        except requests.exceptions.ConnectionError:
                            # vLLMë„ ì—°ê²° ì‹¤íŒ¨ì‹œ ì•ˆë‚´ ë©”ì‹œì§€
                            processing_time = time.time() - start_time
                            fallback_response = {
                                'reply': f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì´ ì‹œì‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {message}\n\ní•œêµ­ë„ë¡œê³µì‚¬ ê´€ë ¨ ë¬¸ì˜ëŠ” ê³ ê°ì„¼í„°(1588-2504)ë¡œ ì—°ë½í•˜ì‹œê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì´ìš©í•´ì£¼ì„¸ìš”.",
                                'sources': [],
                                'status': 'AI ëª¨ë¸ ë¡œë”© ì¤‘',
                                'processing_time': f"{processing_time:.2f}ì´ˆ",
                                'timestamp': datetime.now().isoformat()
                            }
                            log_request('ì±„íŒ…', user_id, 'RAG ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
                            return jsonify(fallback_response)
                        
                        if vllm_response.status_code == 200:
                            vllm_result = vllm_response.json()
                            content = vllm_result['choices'][0]['message']['content']
                            formatted_response = {
                                'reply': content,
                                'sources': [],
                                'status': 'success (vLLM ì§ì ‘ í˜¸ì¶œ)',
                                'route_path': route_path,
                                'confidence': confidence,
                                'reasoning': reasoning,
                                'processing_time': f"{processing_time:.2f}ì´ˆ",
                                'timestamp': datetime.now().isoformat()
                            }
                            
                            # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€ (ìë™ í•„í„°ë§)
                            cleaned_content = add_to_conversation_history(session_id, message, content)
                            formatted_response['reply'] = cleaned_content  # í•„í„°ë§ëœ ì‘ë‹µìœ¼ë¡œ ì—…ë°ì´íŠ¸
                            log_request('ì±„íŒ…', user_id, 'vLLM ì§ì ‘ ì—°ë™', 'success', f"{processing_time:.2f}ì´ˆ")
                            return jsonify(formatted_response)
                except:
                    pass
            
            error_msg = f"RAG API ì˜¤ë¥˜ (HTTP {response.status_code})"
            try:
                error_detail = response.json().get('detail', response.text)
                error_msg += f": {error_detail}"
            except:
                error_msg += f": {response.text}"
            
            log_request('ì±„íŒ…', user_id, 'RAG ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
            
            return jsonify({
                'error': error_msg,
                'status': 'error',
                'processing_time': f"{processing_time:.2f}ì´ˆ"
            }), response.status_code
            
    except requests.exceptions.ConnectionError:
        processing_time = time.time() - start_time
        
        # vLLM ì§ì ‘ ì—°ê²°ë„ ì‹œë„í•´ë³´ê³ , ì‹¤íŒ¨í•˜ë©´ ì•ˆë‚´ ë©”ì‹œì§€
        fallback_response = {
            'reply': f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì´ ì‹œì‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {message}\n\ní•œêµ­ë„ë¡œê³µì‚¬ ê´€ë ¨ ë¬¸ì˜ëŠ” ê³ ê°ì„¼í„°(1588-2504)ë¡œ ì—°ë½í•˜ì‹œê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì´ìš©í•´ì£¼ì„¸ìš”.",
            'sources': [],
            'status': 'AI ëª¨ë¸ ë¡œë”© ì¤‘',
            'processing_time': f"{processing_time:.2f}ì´ˆ",
            'timestamp': datetime.now().isoformat()
        }
        
        log_request('ì±„íŒ…', request.remote_addr, 'ì—°ê²° ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
        return jsonify(fallback_response)
        
    except requests.exceptions.Timeout:
        processing_time = time.time() - start_time
        error_msg = "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        log_request('ì±„íŒ…', request.remote_addr, 'íƒ€ì„ì•„ì›ƒ', 'error', f"{processing_time:.2f}ì´ˆ")
        
        return jsonify({
            'error': error_msg,
            'status': 'timeout',
            'processing_time': f"{processing_time:.2f}ì´ˆ"
        }), 408
        
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        logger.error(f"âŒ ì±„íŒ… í”„ë¡ì‹œ ì˜¤ë¥˜: {str(e)}")
        log_request('ì±„íŒ…', request.remote_addr, 'ë‚´ë¶€ ì˜¤ë¥˜', 'error', f"{processing_time:.2f}ì´ˆ")
        
        return jsonify({
            'error': error_msg,
            'status': 'error',
            'processing_time': f"{processing_time:.2f}ì´ˆ"
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        # RAG ì„œë²„ ìƒíƒœ í™•ì¸
        neoali_status = "unknown"
        try:
            health_response = requests.get("http://localhost:8080/health", timeout=5)
            if health_response.status_code == 200:
                neoali_status = "connected"
            else:
                neoali_status = "error"
        except:
            neoali_status = "disconnected"
        
        return jsonify({
            'status': 'healthy',
            'neoali_rag_status': neoali_status,
            'total_requests': stats_data['total_requests'],
            'active_users': len(stats_data['active_users']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

@app.route('/api/patterns', methods=['GET'])
def get_pattern_analytics():
    """íŒ¨í„´ ë¶„ì„ ë° ê´€ë¦¬ API"""
    try:
        # íŒ¨í„´ ì„±ëŠ¥ í†µê³„
        performance_stats = {}
        for pattern_key, data in pattern_learning_data['pattern_performance'].items():
            success_rate = (data['success'] / data['count'] * 100) if data['count'] > 0 else 0
            performance_stats[pattern_key] = {
                'count': data['count'],
                'success': data['success'],
                'success_rate': f"{success_rate:.1f}%"
            }
        
        # ìƒˆë¡œìš´ íŒ¨í„´ ì œì•ˆ
        suggestions = suggest_new_patterns()
        
        # ìµœê·¼ ë¼ìš°íŒ… ì˜¤ë¥˜
        recent_errors = pattern_learning_data['misrouted_queries'][-10:]
        
        return jsonify({
            'pattern_performance': performance_stats,
            'pattern_suggestions': suggestions,
            'recent_misrouted': recent_errors,
            'last_update': pattern_learning_data['last_update'].isoformat(),
            'total_queries': sum(data['count'] for data in pattern_learning_data['pattern_performance'].values())
        })
        
    except Exception as e:
        logger.error(f"íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/api/patterns/feedback', methods=['POST'])
def submit_pattern_feedback():
    """íŒ¨í„´ í”¼ë“œë°± ì œì¶œ API"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        expected_path = data.get('expected_path', '')
        actual_path = data.get('actual_path', '')
        
        # í”¼ë“œë°± ë°ì´í„° ìˆ˜ì§‘
        collect_misrouted_query(query, expected_path, actual_path)
        
        logger.info(f"ğŸ“ íŒ¨í„´ í”¼ë“œë°± ìˆ˜ì§‘: {query} -> ì˜ˆìƒ:{expected_path}, ì‹¤ì œ:{actual_path}")
        
        return jsonify({
            'status': 'success',
            'message': 'í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
        
    except Exception as e:
        logger.error(f"íŒ¨í„´ í”¼ë“œë°± ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/api/stats', methods=['GET'])
def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ë°ì´í„° ë°˜í™˜"""
    try:
        total = stats_data['total_requests']
        success_rate = (stats_data['successful_requests'] / total * 100) if total > 0 else 100
        
        return jsonify({
            'total_requests': stats_data['total_requests'],
            'successful_requests': stats_data['successful_requests'], 
            'failed_requests': stats_data['failed_requests'],
            'success_rate': f"{success_rate:.1f}%",
            'active_users': len(stats_data['active_users']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"í†µê³„ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/test', methods=['GET']) 
def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        "message": "ex-GPT + RAG ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.", 
        "neoali_integration": True,
        "endpoints": [
            "/api/chat",
            "/api/health", 
            "/api/stats",
            "/api/test"
        ]
    })

# ì˜¤ë¥˜ í•¸ë“¤ëŸ¬
@app.route('/favicon.ico')
def favicon():
    return '', 204

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Endpoint not found'}), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(error)}")
    return jsonify({'error': 'Internal server error'}), 500

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ 
# ============================================================================

if __name__ == '__main__':
    logger.info("ğŸš€ ex-GPT í†µí•© ì„œë²„ ì‹œì‘...")
    logger.info("ğŸ”— RAG API: http://localhost:8081")
    logger.info("ğŸ“ ì›¹ ì„œë²„: http://localhost:5001") 
    logger.info("ğŸ‰ ì¤€ë¹„ ì™„ë£Œ!")
    
    app.run(host='0.0.0.0', port=5001, debug=False, threaded=True)