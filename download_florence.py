import torch
from transformers import AutoProcessor, AutoModelForCausalLM
import os

def download_florence_model():
    print("Florence-2-large ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
    model_path = "./models/florence-2-large"
    os.makedirs(model_path, exist_ok=True)
    
    try:
        # í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ
        print("1. í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        processor = AutoProcessor.from_pretrained(
            "microsoft/Florence-2-large",
            trust_remote_code=True,
            cache_dir=model_path
        )
        print("âœ… í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print("2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            "microsoft/Florence-2-large",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True,
            cache_dir=model_path
        )
        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # ë‹¤ìš´ë¡œë“œ í™•ì¸
        print("3. ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        
        print(f"âœ… ëª¨ë¸ì´ {device}ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

if __name__ == "__main__":
    success = download_florence_model()
    if success:
        print("\nğŸ‰ Florence-2-large ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ!")
    else:
        print("\nğŸ’¥ ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ê³¼ ë””ìŠ¤í¬ ê³µê°„ì„ í™•ì¸í•˜ì„¸ìš”.")